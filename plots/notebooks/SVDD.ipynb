{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d3da07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import geoopt\n",
    "\n",
    "\n",
    "def pairwise_inner(x: Tensor, y: Tensor, curv: float | Tensor = 1.0):\n",
    "    x_time = torch.sqrt(1 / curv + torch.sum(x**2, dim=-1, keepdim=True))\n",
    "    y_time = torch.sqrt(1 / curv + torch.sum(y**2, dim=-1, keepdim=True))\n",
    "    xyl = x @ y.T - x_time @ y_time.T\n",
    "    return xyl\n",
    "\n",
    "\n",
    "def pairwise_dist(\n",
    "    x: Tensor, y: Tensor, curv: float | Tensor = 1.0, eps: float = 1e-8\n",
    ") -> Tensor:\n",
    "    c_xyl = -curv * pairwise_inner(x, y, curv)\n",
    "    _distance = torch.acosh(torch.clamp(c_xyl, min=1 + eps))\n",
    "    return _distance / curv**0.1\n",
    "\n",
    "\n",
    "def elementwise_inner(x: Tensor, y: Tensor, curv: float | Tensor = 1.0):\n",
    "    x_time = torch.sqrt(1 / curv + torch.sum(x**2, dim=-1))\n",
    "    y_time = torch.sqrt(1 / curv + torch.sum(y**2, dim=-1))\n",
    "    xyl = torch.sum(x * y, dim=-1) - x_time * y_time\n",
    "    return xyl\n",
    "\n",
    "\n",
    "def elementwise_dist(\n",
    "    x: Tensor, y: Tensor, curv: float | Tensor = 1.0, eps: float = 1e-8\n",
    ") -> Tensor:\n",
    "    c_xyl = -curv * elementwise_inner(x, y, curv)\n",
    "    _distance = torch.acosh(torch.clamp(c_xyl, min=1 + eps))\n",
    "    return _distance / curv**0.1\n",
    "\n",
    "\n",
    "def exp_map0(x: Tensor, curv: float | Tensor = 1.0, eps: float = 1e-8) -> Tensor:\n",
    "    if torch.isnan(x).any() or torch.isinf(x).any():\n",
    "        print(\"NaN or Inf detected in input to exp_map0\")\n",
    "\n",
    "    x_norm = torch.norm(x, dim=-1, keepdim=True)\n",
    "    rc_xnorm = curv**0.1 * x_norm\n",
    "\n",
    "    sinh_input = torch.clamp(rc_xnorm, min=eps, max=math.asinh(2**15))\n",
    "    rc_xnorm_clamped = torch.clamp(rc_xnorm, min=eps)\n",
    "\n",
    "    _output = torch.sinh(sinh_input) * x / rc_xnorm_clamped\n",
    "\n",
    "    if torch.isnan(_output).any() or torch.isinf(_output).any():\n",
    "        print(\"NaN or Inf detected in output of exp_map0\")\n",
    "\n",
    "    return _output\n",
    "\n",
    "\n",
    "def log_map0(x: Tensor, curv: float | Tensor = 1.0, eps: float = 1e-5) -> Tensor:\n",
    "    rc_x_time = torch.sqrt(1 + curv * torch.sum(x**2, dim=-1, keepdim=True))\n",
    "    _distance0 = torch.acosh(torch.clamp(rc_x_time, min=1 + eps))\n",
    "\n",
    "    rc_xnorm = curv**0.1 * torch.norm(x, dim=-1, keepdim=True)\n",
    "    _output = _distance0 * x / torch.clamp(rc_xnorm, min=eps)\n",
    "    return _output\n",
    "\n",
    "\n",
    "def half_aperture(\n",
    "    x: Tensor, curv: float | Tensor = 1.0, min_radius: float = 0.1, eps: float = 1e-5\n",
    ") -> Tensor:\n",
    "    asin_input = 2 * min_radius / (torch.norm(x, dim=-1) * curv**0.1 + eps)\n",
    "    _half_aperture = torch.asin(torch.clamp(asin_input, min=-1 + eps, max=1 - eps))\n",
    "\n",
    "    return _half_aperture\n",
    "\n",
    "\n",
    "def oxy_angle(x: Tensor, y: Tensor, curv: float | Tensor = 1.0, eps: float = 1e-5):\n",
    "    # Calculate time components of inputs (multiplied with `sqrt(curv)`):\n",
    "    x_time = torch.sqrt(1 / curv + torch.sum(x**2, dim=-1))\n",
    "    y_time = torch.sqrt(1 / curv + torch.sum(y**2, dim=-1))\n",
    "\n",
    "    # Calculate lorentzian inner product multiplied with curvature. We do not use\n",
    "    # the `pairwise_inner` implementation to save some operations (since we only\n",
    "    # need the diagonal elements).\n",
    "    c_xyl = curv * (torch.sum(x * y, dim=-1) - x_time * y_time)\n",
    "\n",
    "    # Make the numerator and denominator for input to arc-cosh, shape: (B, )\n",
    "    acos_numer = y_time + c_xyl * x_time\n",
    "    acos_denom = torch.sqrt(torch.clamp(c_xyl**2 - 1, min=eps))\n",
    "\n",
    "    acos_input = acos_numer / (torch.norm(x, dim=-1) * acos_denom + eps)\n",
    "    _angle = torch.acos(torch.clamp(acos_input, min=-1 + eps, max=1 - eps))\n",
    "\n",
    "    return _angle\n",
    "\n",
    "\n",
    "def hyperbolic_distance(\n",
    "    x: Tensor, y: Tensor, curv: float | Tensor = 1.0, eps: float = 1e-8\n",
    ") -> Tensor:\n",
    "    inner_prod = -x[0] * y[0] + torch.dot(x[1:], y[1:])\n",
    "    val = torch.clamp(-inner_prod, min=1.0 + eps)\n",
    "    dist = torch.sqrt(torch.tensor(curv)) * torch.acosh(val)\n",
    "    return dist\n",
    "\n",
    "\n",
    "# def batch_hyperbolic_distance(\n",
    "#     x: Tensor, y: Tensor, curv: float | Tensor = 1.0, eps: float = 1e-8\n",
    "# ) -> Tensor:\n",
    "#     if x.shape[0] != y.shape[0]:\n",
    "#         raise ValueError(\"Input tensors must have the same batch size.\")\n",
    "#     distances = []\n",
    "#     for i in range(x.shape[0]):\n",
    "#         distances.append(hyperbolic_distance(x[i], y[i], curv, eps))\n",
    "\n",
    "#     return torch.stack(distances)\n",
    "\n",
    "\n",
    "def lorentz_inner_product(x, y):\n",
    "    # x: (..., d+1), y: (..., d+1) or (1, d+1)\n",
    "    return -x[..., 0] * y[..., 0] + torch.sum(x[..., 1:] * y[..., 1:], dim=-1)\n",
    "\n",
    "\n",
    "def batch_hyperbolic_distance(x, y, curv=1.0, eps=1e-5, max_acosh=1e6):\n",
    "    ip = lorentz_inner_product(x, y)\n",
    "    # Clamp both lower and upper bounds\n",
    "    val = torch.clamp(-ip, min=1.0 + eps, max=max_acosh)\n",
    "    dist = torch.sqrt(torch.tensor(curv, device=x.device, dtype=x.dtype)) * torch.acosh(\n",
    "        val\n",
    "    )\n",
    "    return dist\n",
    "\n",
    "\n",
    "def is_lorentz_point(x, curv=1.0, tol=1e-4):\n",
    "    # Returns True if x is (almost) on the Lorentz hyperboloid\n",
    "    norm = -x[..., 0] ** 2 + torch.sum(x[..., 1:] ** 2, dim=-1)\n",
    "    return (torch.abs(norm - 1.0 / curv) < tol).all()\n",
    "\n",
    "\n",
    "def project_to_lorentz(x, curv=1.0):\n",
    "    space = x[..., 1:]\n",
    "    t = torch.sqrt(1.0 / curv + torch.sum(space**2, dim=-1, keepdim=True))\n",
    "    return torch.cat([t, space], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5def759b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of benign points: torch.Size([158700, 768])\n"
     ]
    }
   ],
   "source": [
    "# load the hyperbolic embeddings from the file\n",
    "hyperbolic_path = \"/mnt/ssd1/mary/Diffusion-Models-Embedding-Space-Defense/hyperbolic_safe_clip/visu_validation/03f7a6e1816195a039adf08998aa1691_all_embeddings.pt\"\n",
    "hyperbolic_points = torch.load(hyperbolic_path)\n",
    "\n",
    "\n",
    "# get only the points whose class is 'benign'\n",
    "bening_point = []\n",
    "for point in hyperbolic_points:\n",
    "    if point[1] == \"benign\":\n",
    "        bening_point.append(point[0])\n",
    "\n",
    "\n",
    "benign_points = torch.stack(bening_point)\n",
    "print(f\"Number of benign points: {benign_points.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b6e5a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geoopt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "\n",
    "class LorentzHyperbolicSVDD:\n",
    "    def __init__(\n",
    "        self,\n",
    "        curvature=1.0,\n",
    "        radius_init=1.0,\n",
    "        center_lr=0.02,\n",
    "        radius_lr=0.01,\n",
    "        nu=0.1,\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "        self.curvature = curvature\n",
    "        self.radius = radius_init\n",
    "        self.center_lr = center_lr\n",
    "        self.radius_lr = radius_lr\n",
    "        self.device = device\n",
    "        self.nu = nu\n",
    "\n",
    "    def loss_SVDD(self, x, center, radius):\n",
    "        center_batch = center.unsqueeze(0).expand(x.shape[0], -1)\n",
    "        distances_sq = (\n",
    "            batch_hyperbolic_distance(x, center_batch, curv=self.curvature) ** 2\n",
    "        )\n",
    "        penalty = torch.relu(distances_sq - radius**2)\n",
    "        loss = radius**2 + torch.mean(penalty) / self.nu\n",
    "        return loss\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        x,\n",
    "        epochs: int = 100,\n",
    "        batch_size: int = 32,\n",
    "        center_lr: float = 0.02,\n",
    "        radius_lr: float = 0.01,\n",
    "    ):\n",
    "        # Prepare data with time component (in minibatches)\n",
    "        mean_center = torch.mean(x, dim=0)\n",
    "        print(f\"Mean center before adding time component: {mean_center.shape}\")\n",
    "        x = torch.cat(\n",
    "            [torch.sqrt(1 / self.curvature + torch.sum(x**2, dim=-1, keepdim=True)), x],\n",
    "            dim=-1,\n",
    "        )\n",
    "        x = x.to(self.device)\n",
    "        print(\"data after adding time component:\", x.shape)\n",
    "        mean_center = torch.cat(\n",
    "            [\n",
    "                torch.sqrt(\n",
    "                    1 / self.curvature + torch.sum(mean_center**2, dim=-1, keepdim=True)\n",
    "                ),\n",
    "                mean_center,\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "        dataloader = DataLoader(TensorDataset(x), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.center_param = geoopt.ManifoldParameter(\n",
    "            mean_center.clone().detach().to(self.device),\n",
    "            manifold=geoopt.Lorentz(k=self.curvature),\n",
    "        )\n",
    "\n",
    "        radius_init = torch.tensor(self.radius, device=self.device)\n",
    "        self.radius_param = torch.nn.Parameter(\n",
    "            radius_init.clone().detach().to(self.device)\n",
    "        )\n",
    "\n",
    "        center_optimizer = geoopt.optim.RiemannianSGD(\n",
    "            params=[self.center_param], lr=center_lr\n",
    "        )\n",
    "        radius_optimizer = torch.optim.SGD(\n",
    "            [{\"params\": self.radius_param, \"lr\": radius_lr}]\n",
    "        )\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            total_inside = 0\n",
    "            total_seen = 0\n",
    "            for batch in dataloader:\n",
    "                batch_x = batch[0]\n",
    "                center_optimizer.zero_grad()\n",
    "                radius_optimizer.zero_grad()\n",
    "                loss = self.loss_SVDD(batch_x, self.center_param, self.radius_param)\n",
    "                loss.backward()\n",
    "                center_optimizer.step()\n",
    "                radius_optimizer.step()\n",
    "                epoch_loss += loss.item() * batch_x.size(\n",
    "                    0\n",
    "                )  # accumulate (not average) for the epoch\n",
    "\n",
    "                # Minibatch stats\n",
    "                center_batch = self.center_param.unsqueeze(0).expand(\n",
    "                    batch_x.shape[0], -1\n",
    "                )\n",
    "                distances = batch_hyperbolic_distance(\n",
    "                    batch_x, center_batch, curv=self.curvature\n",
    "                )\n",
    "                inside_count = torch.sum(distances <= self.radius_param).item()\n",
    "                total_inside += inside_count\n",
    "                total_seen += batch_x.size(0)\n",
    "\n",
    "            avg_loss = epoch_loss / total_seen\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{epochs}], Avg Loss: {avg_loss:.4f}, Points inside radius (minibatch stats): {total_inside}/{total_seen}, center norm: {self.center_param.norm().item():.4f}, radius: {self.radius_param.item():.4f}\"\n",
    "            )\n",
    "\n",
    "    def fit_alternatively(\n",
    "        self,\n",
    "        x,\n",
    "        epochs: int = 100,\n",
    "        batch_size: int = 1024,\n",
    "        epoch_center: int = 10,\n",
    "        epoch_radius: int = 5,\n",
    "        center_lr: float = 0.02,\n",
    "        radius_lr: float = 0.01,\n",
    "    ):\n",
    "        # Compute mean center before time component\n",
    "        mean_center = torch.mean(x, dim=0)\n",
    "        print(f\"Mean center before adding time component: {mean_center.shape}\")\n",
    "        # Add time component to dataset and mean center\n",
    "        x = torch.cat(\n",
    "            [torch.sqrt(1 / self.curvature + torch.sum(x**2, dim=-1, keepdim=True)), x],\n",
    "            dim=-1,\n",
    "        )\n",
    "        x = x.to(self.device)\n",
    "        print(\"data after adding time component:\", x.shape)\n",
    "        mean_center = torch.cat(\n",
    "            [\n",
    "                torch.sqrt(\n",
    "                    1 / self.curvature + torch.sum(mean_center**2, dim=-1, keepdim=True)\n",
    "                ),\n",
    "                mean_center,\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "        dataloader = DataLoader(TensorDataset(x), batch_size=batch_size, shuffle=True)\n",
    "        # Use mean center as initialization\n",
    "        self.center_param = geoopt.ManifoldParameter(\n",
    "            mean_center.clone().detach().to(self.device),\n",
    "            manifold=geoopt.Lorentz(k=self.curvature),\n",
    "        )\n",
    "        radius_init = torch.tensor(self.radius, device=self.device)\n",
    "        self.radius_param = torch.nn.Parameter(\n",
    "            radius_init.clone().detach().to(self.device)\n",
    "        )\n",
    "\n",
    "        center_optimizer = geoopt.optim.RiemannianSGD(\n",
    "            params=[self.center_param], lr=center_lr\n",
    "        )\n",
    "        radius_optimizer = torch.optim.SGD(\n",
    "            [{\"params\": self.radius_param, \"lr\": radius_lr}]\n",
    "        )\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            total_inside = 0\n",
    "            total_seen = 0\n",
    "            if epoch % (epoch_center + epoch_radius) < epoch_center:\n",
    "                # Optimize center only\n",
    "                for batch in dataloader:\n",
    "                    batch_x = batch[0]\n",
    "                    center_optimizer.zero_grad()\n",
    "                    loss = self.loss_SVDD(batch_x, self.center_param, self.radius_param)\n",
    "                    loss.backward()\n",
    "                    center_optimizer.step()\n",
    "                    epoch_loss += loss.item() * batch_x.size(0)\n",
    "                    # Minibatch stats\n",
    "                    center_batch = self.center_param.unsqueeze(0).expand(\n",
    "                        batch_x.shape[0], -1\n",
    "                    )\n",
    "                    distances = batch_hyperbolic_distance(\n",
    "                        batch_x, center_batch, curv=self.curvature\n",
    "                    )\n",
    "                    inside_count = torch.sum(distances <= self.radius_param).item()\n",
    "                    total_inside += inside_count\n",
    "                    total_seen += batch_x.size(0)\n",
    "            else:\n",
    "                # Optimize radius only\n",
    "                for batch in dataloader:\n",
    "                    batch_x = batch[0]\n",
    "                    radius_optimizer.zero_grad()\n",
    "                    loss = self.loss_SVDD(batch_x, self.center_param, self.radius_param)\n",
    "                    loss.backward()\n",
    "                    radius_optimizer.step()\n",
    "                    epoch_loss += loss.item() * batch_x.size(0)\n",
    "                    # Minibatch stats\n",
    "                    center_batch = self.center_param.unsqueeze(0).expand(\n",
    "                        batch_x.shape[0], -1\n",
    "                    )\n",
    "                    distances = batch_hyperbolic_distance(\n",
    "                        batch_x, center_batch, curv=self.curvature\n",
    "                    )\n",
    "                    inside_count = torch.sum(distances <= self.radius_param).item()\n",
    "                    total_inside += inside_count\n",
    "                    total_seen += batch_x.size(0)\n",
    "\n",
    "            avg_loss = epoch_loss / total_seen\n",
    "            # Optionally print gradient norms if you want\n",
    "            center_grad_norm = (\n",
    "                self.center_param.grad.norm().item()\n",
    "                if self.center_param.grad is not None\n",
    "                else 0.0\n",
    "            )\n",
    "            radius_grad_norm = (\n",
    "                self.radius_param.grad.norm().item()\n",
    "                if self.radius_param.grad is not None\n",
    "                else 0.0\n",
    "            )\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{epochs}], Avg Loss: {avg_loss:.4f}, center: {self.center_param.norm().item():.4f}, radius: {self.radius_param.item():.4f}, inside: {total_inside}/{total_seen}, center_grad_norm: {center_grad_norm:.4f}, radius_grad_norm: {radius_grad_norm:.4f}\"\n",
    "            )\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            distances = batch_hyperbolic_distance(\n",
    "                x, self.center_param, curv=self.curvature\n",
    "            )\n",
    "            predictions = (distances <= self.radius_param).int()\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "899a36fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_svdd_fit(hyper_points, nu, curvature=1.0, epochs=500):\n",
    "    num_tot = hyper_points.shape[0]\n",
    "    model = LorentzHyperbolicSVDD(\n",
    "        curvature=curvature, center_lr=0.1, radius_lr=0.2, nu=nu\n",
    "    )\n",
    "\n",
    "    print(\"Before fit:\")\n",
    "\n",
    "    model.fit(hyper_points, epochs=epochs)\n",
    "\n",
    "    print(\"After fit:\")\n",
    "    print(\"Center:\", model.center_param)\n",
    "    print(\"Radius:\", model.radius_param.item())\n",
    "\n",
    "    # add the time component to the hyperbolic points\n",
    "    hyper_points = torch.cat(\n",
    "        [\n",
    "            torch.sqrt(\n",
    "                1 / model.curvature + torch.sum(hyper_points**2, dim=-1, keepdim=True)\n",
    "            ),\n",
    "            hyper_points,\n",
    "        ],\n",
    "        dim=-1,\n",
    "    )\n",
    "    center_batch = model.center_param.expand(hyper_points.shape[0], -1)\n",
    "\n",
    "    dists = batch_hyperbolic_distance(hyper_points, center_batch, curv=model.curvature)\n",
    "    print(\"Distances to center:\", dists)\n",
    "    print(\"Max distance:\", dists.max().item())\n",
    "    print(\"Radius:\", model.radius_param.item())\n",
    "    # assert (dists <= model.radius_param.item() + 1e-2).all(), \"Not all points inside radius after fit\"\n",
    "    inner_points = (dists <= model.radius_param.item()).float()\n",
    "    count_inner = inner_points.sum().item()\n",
    "    print(f\"Number of points inside radius: {count_inner}/{num_tot}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def test_svdd_fit_alternatively(hyper_points, nu, curvature=1.0, epochs=500):\n",
    "    num_tot = hyper_points.shape[0]\n",
    "    model = LorentzHyperbolicSVDD(\n",
    "        curvature=curvature, center_lr=0.1, radius_lr=0.2, nu=nu\n",
    "    )\n",
    "\n",
    "    print(\"Before fit:\")\n",
    "    model.fit_alternatively(hyper_points, epochs=epochs)\n",
    "\n",
    "    print(\"After fit:\")\n",
    "    print(\"Center:\", model.center_param)\n",
    "    print(\"Radius:\", model.radius_param.item())\n",
    "\n",
    "    # add the time component to the hyperbolic points\n",
    "    hyper_points = torch.cat(\n",
    "        [\n",
    "            torch.sqrt(\n",
    "                1 / model.curvature + torch.sum(hyper_points**2, dim=-1, keepdim=True)\n",
    "            ),\n",
    "            hyper_points,\n",
    "        ],\n",
    "        dim=-1,\n",
    "    )\n",
    "    center_batch = model.center_param.expand(hyper_points.shape[0], -1)\n",
    "\n",
    "    dists = batch_hyperbolic_distance(hyper_points, center_batch, curv=model.curvature)\n",
    "    print(\"Distances to center:\", dists)\n",
    "    print(\"Max distance:\", dists.max().item())\n",
    "    print(\"Radius:\", model.radius_param.item())\n",
    "    # assert (dists <= model.radius_param.item() + 1e-2).all(), \"Not all points inside radius after fit\"\n",
    "    inner_points = (dists <= model.radius_param.item()).float()\n",
    "    count_inner = inner_points.sum().item()\n",
    "    print(f\"Number of points inside radius: {count_inner}/{num_tot}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b10be462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before fit:\n",
      "Mean center before adding time component: torch.Size([768])\n",
      "data after adding time component: torch.Size([158700, 769])\n",
      "Epoch [1/50], Avg Loss: 0.4335, Points inside radius (minibatch stats): 153660/158700, center norm: 1.9184, radius: 0.6231\n",
      "Epoch [2/50], Avg Loss: 0.4315, Points inside radius (minibatch stats): 153754/158700, center norm: 1.9430, radius: 0.6033\n",
      "Epoch [3/50], Avg Loss: 0.4315, Points inside radius (minibatch stats): 153578/158700, center norm: 1.9242, radius: 0.6134\n",
      "Epoch [4/50], Avg Loss: 0.4316, Points inside radius (minibatch stats): 153639/158700, center norm: 1.9347, radius: 0.6091\n",
      "Epoch [5/50], Avg Loss: 0.4317, Points inside radius (minibatch stats): 153763/158700, center norm: 1.9399, radius: 0.6417\n",
      "Epoch [6/50], Avg Loss: 0.4316, Points inside radius (minibatch stats): 153694/158700, center norm: 1.9268, radius: 0.6006\n",
      "Epoch [7/50], Avg Loss: 0.4318, Points inside radius (minibatch stats): 153709/158700, center norm: 1.9225, radius: 0.6137\n",
      "Epoch [8/50], Avg Loss: 0.4317, Points inside radius (minibatch stats): 153817/158700, center norm: 1.9372, radius: 0.5938\n",
      "Epoch [9/50], Avg Loss: 0.4318, Points inside radius (minibatch stats): 153627/158700, center norm: 1.9269, radius: 0.6199\n",
      "Epoch [10/50], Avg Loss: 0.4313, Points inside radius (minibatch stats): 153647/158700, center norm: 1.9236, radius: 0.6126\n",
      "Epoch [11/50], Avg Loss: 0.4317, Points inside radius (minibatch stats): 153662/158700, center norm: 1.9349, radius: 0.6113\n",
      "Epoch [12/50], Avg Loss: 0.4315, Points inside radius (minibatch stats): 153562/158700, center norm: 1.9171, radius: 0.6570\n",
      "Epoch [13/50], Avg Loss: 0.4315, Points inside radius (minibatch stats): 153553/158700, center norm: 1.9155, radius: 0.6130\n",
      "Epoch [14/50], Avg Loss: 0.4315, Points inside radius (minibatch stats): 153690/158700, center norm: 1.9210, radius: 0.6082\n",
      "Epoch [15/50], Avg Loss: 0.4318, Points inside radius (minibatch stats): 153567/158700, center norm: 1.9356, radius: 0.6103\n",
      "Epoch [16/50], Avg Loss: 0.4318, Points inside radius (minibatch stats): 153741/158700, center norm: 1.9213, radius: 0.6139\n",
      "Epoch [17/50], Avg Loss: 0.4316, Points inside radius (minibatch stats): 153590/158700, center norm: 1.9529, radius: 0.6372\n",
      "Epoch [18/50], Avg Loss: 0.4313, Points inside radius (minibatch stats): 153630/158700, center norm: 1.9289, radius: 0.6287\n",
      "Epoch [19/50], Avg Loss: 0.4315, Points inside radius (minibatch stats): 153618/158700, center norm: 1.9477, radius: 0.6034\n",
      "Epoch [20/50], Avg Loss: 0.4317, Points inside radius (minibatch stats): 153548/158700, center norm: 1.9270, radius: 0.6315\n",
      "Epoch [21/50], Avg Loss: 0.4319, Points inside radius (minibatch stats): 153632/158700, center norm: 1.9684, radius: 0.6586\n",
      "Epoch [22/50], Avg Loss: 0.4316, Points inside radius (minibatch stats): 153605/158700, center norm: 1.9567, radius: 0.6670\n",
      "Epoch [23/50], Avg Loss: 0.4316, Points inside radius (minibatch stats): 153623/158700, center norm: 1.9775, radius: 0.6444\n",
      "Epoch [24/50], Avg Loss: 0.4316, Points inside radius (minibatch stats): 153542/158700, center norm: 1.9308, radius: 0.6215\n",
      "Epoch [25/50], Avg Loss: 0.4321, Points inside radius (minibatch stats): 153639/158700, center norm: 1.9280, radius: 0.6154\n",
      "Epoch [26/50], Avg Loss: 0.4315, Points inside radius (minibatch stats): 153717/158700, center norm: 1.9584, radius: 0.6114\n",
      "Epoch [27/50], Avg Loss: 0.4314, Points inside radius (minibatch stats): 153714/158700, center norm: 1.9497, radius: 0.6480\n",
      "Epoch [28/50], Avg Loss: 0.4317, Points inside radius (minibatch stats): 153660/158700, center norm: 1.9362, radius: 0.6341\n",
      "Epoch [29/50], Avg Loss: 0.4320, Points inside radius (minibatch stats): 153700/158700, center norm: 1.9421, radius: 0.6193\n",
      "Epoch [30/50], Avg Loss: 0.4316, Points inside radius (minibatch stats): 153763/158700, center norm: 1.9600, radius: 0.6549\n",
      "Epoch [31/50], Avg Loss: 0.4316, Points inside radius (minibatch stats): 153708/158700, center norm: 1.9343, radius: 0.6599\n",
      "Epoch [32/50], Avg Loss: 0.4317, Points inside radius (minibatch stats): 153688/158700, center norm: 1.9266, radius: 0.6038\n",
      "Epoch [33/50], Avg Loss: 0.4319, Points inside radius (minibatch stats): 153747/158700, center norm: 1.9542, radius: 0.6212\n",
      "Epoch [34/50], Avg Loss: 0.4316, Points inside radius (minibatch stats): 153654/158700, center norm: 1.9362, radius: 0.6443\n",
      "Epoch [35/50], Avg Loss: 0.4318, Points inside radius (minibatch stats): 153686/158700, center norm: 1.9546, radius: 0.6284\n",
      "Epoch [36/50], Avg Loss: 0.4317, Points inside radius (minibatch stats): 153559/158700, center norm: 1.9571, radius: 0.6150\n",
      "Epoch [37/50], Avg Loss: 0.4317, Points inside radius (minibatch stats): 153612/158700, center norm: 1.9328, radius: 0.6300\n",
      "Epoch [38/50], Avg Loss: 0.4313, Points inside radius (minibatch stats): 153685/158700, center norm: 1.9352, radius: 0.6052\n",
      "Epoch [39/50], Avg Loss: 0.4315, Points inside radius (minibatch stats): 153618/158700, center norm: 1.9616, radius: 0.6456\n",
      "Epoch [40/50], Avg Loss: 0.4317, Points inside radius (minibatch stats): 153579/158700, center norm: 1.9654, radius: 0.6466\n",
      "Epoch [41/50], Avg Loss: 0.4317, Points inside radius (minibatch stats): 153646/158700, center norm: 1.9254, radius: 0.6537\n",
      "Epoch [42/50], Avg Loss: 0.4321, Points inside radius (minibatch stats): 153485/158700, center norm: 1.9481, radius: 0.6268\n",
      "Epoch [43/50], Avg Loss: 0.4314, Points inside radius (minibatch stats): 153615/158700, center norm: 1.9413, radius: 0.6175\n",
      "Epoch [44/50], Avg Loss: 0.4317, Points inside radius (minibatch stats): 153636/158700, center norm: 1.9556, radius: 0.6348\n",
      "Epoch [45/50], Avg Loss: 0.4316, Points inside radius (minibatch stats): 153619/158700, center norm: 1.9302, radius: 0.5955\n",
      "Epoch [46/50], Avg Loss: 0.4314, Points inside radius (minibatch stats): 153617/158700, center norm: 1.9247, radius: 0.6464\n",
      "Epoch [47/50], Avg Loss: 0.4318, Points inside radius (minibatch stats): 153631/158700, center norm: 1.9394, radius: 0.6394\n",
      "Epoch [48/50], Avg Loss: 0.4317, Points inside radius (minibatch stats): 153582/158700, center norm: 1.9398, radius: 0.6908\n",
      "Epoch [49/50], Avg Loss: 0.4315, Points inside radius (minibatch stats): 153587/158700, center norm: 1.9735, radius: 0.6302\n",
      "Epoch [50/50], Avg Loss: 0.4315, Points inside radius (minibatch stats): 153675/158700, center norm: 1.9790, radius: 0.6320\n",
      "After fit:\n",
      "Center: Parameter on Lorentz manifold containing:\n",
      "Parameter(ManifoldParameter([ 1.7634e+00, -2.1183e-02, -4.0871e-03,  2.0812e-02,\n",
      "                    4.7659e-02,  1.7881e-02,  1.0863e-02,  1.5279e-02,\n",
      "                   -2.0411e-02, -1.3596e-02, -2.0224e-03,  2.3057e-02,\n",
      "                   -2.3665e-02, -1.3224e-02, -9.2321e-03,  1.7966e-02,\n",
      "                   -3.5929e-02,  3.3576e-02, -1.9454e-03,  1.2148e-02,\n",
      "                   -3.7031e-02,  3.5025e-05, -1.3194e-02, -5.6440e-03,\n",
      "                    4.4040e-02, -5.1902e-02,  1.1838e-02,  5.5897e-03,\n",
      "                    2.0522e-02,  4.3413e-02,  4.7515e-03, -2.7236e-03,\n",
      "                    1.2377e-02, -3.5717e-03, -1.0159e-02,  4.0189e-03,\n",
      "                   -7.5126e-03, -1.7113e-02, -2.0225e-03,  5.7411e-03,\n",
      "                   -8.4172e-03,  4.3613e-02, -3.4150e-02,  2.5925e-02,\n",
      "                   -5.9653e-02,  3.3639e-02, -2.0485e-02,  4.2210e-02,\n",
      "                    1.5105e-03, -1.5349e-02, -9.0167e-02, -1.5902e-02,\n",
      "                    4.2759e-03,  1.1062e-02,  1.9955e-02,  1.8454e-02,\n",
      "                    7.4954e-03,  2.3249e-03,  1.8922e-02,  3.6148e-03,\n",
      "                   -4.1966e-03,  1.8447e-02,  3.2653e-02, -5.9606e-03,\n",
      "                    2.0983e-02,  6.4029e-03,  1.8532e-02,  7.4782e-03,\n",
      "                    8.7368e-03, -8.2402e-04, -2.5819e-02,  9.9056e-03,\n",
      "                    3.7467e-02,  2.2369e-03, -1.3470e-02,  1.1323e-02,\n",
      "                   -4.1301e-03,  8.7912e-03,  1.2613e-03,  1.1823e-02,\n",
      "                   -2.0750e-02,  4.7497e-02, -1.7319e-02,  2.1446e-02,\n",
      "                   -7.1459e-03,  2.0889e-02,  6.9292e-03, -3.2745e-02,\n",
      "                   -4.6658e-04,  2.6559e-02,  3.4194e-02, -2.8679e-03,\n",
      "                   -5.9969e-03, -9.6595e-04, -3.9474e-02,  2.8502e-03,\n",
      "                   -3.1106e-03,  3.7864e-02, -3.9405e-02,  3.0083e-02,\n",
      "                   -2.3396e-02, -4.2174e-02,  2.3834e-02, -1.3128e-02,\n",
      "                   -5.5832e-04, -3.6132e-02,  5.0681e-03, -2.4288e-02,\n",
      "                   -2.7628e-02, -3.4495e-03, -2.0677e-02,  4.1325e-02,\n",
      "                    6.8906e-04, -9.8196e-03, -6.3408e-03, -1.7398e-02,\n",
      "                   -2.0817e-02, -1.0874e-02, -8.0334e-03, -9.0013e-03,\n",
      "                   -2.4501e-03,  4.2032e-02, -7.2343e-03,  1.7756e-03,\n",
      "                    9.5603e-03, -4.0897e-03, -3.4083e-02,  1.7111e-02,\n",
      "                    1.7094e-02,  2.3686e-02, -1.6551e-02,  3.1875e-03,\n",
      "                    3.1978e-02, -4.6680e-03, -1.3603e-02,  2.5029e-02,\n",
      "                   -1.6934e-02, -2.1637e-02,  5.4064e-03,  9.5019e-03,\n",
      "                    3.5157e-03,  2.0900e-02, -2.3985e-02,  2.6010e-02,\n",
      "                   -3.9985e-02, -1.4471e-02,  2.3594e-02,  5.3311e-02,\n",
      "                   -3.5579e-02, -3.4052e-02,  2.9806e-02,  1.0986e-02,\n",
      "                    2.6294e-02,  4.5663e-02,  2.8076e-02,  3.8177e-02,\n",
      "                    3.7892e-02,  2.0703e-02,  3.8507e-03,  2.1225e-02,\n",
      "                   -1.9013e-02,  1.9253e-02,  2.3320e-03,  2.0223e-02,\n",
      "                    5.5530e-02,  3.6015e-02,  7.8236e-03, -4.0468e-02,\n",
      "                   -4.5221e-02, -3.8070e-03,  2.6238e-03,  3.8780e-03,\n",
      "                   -3.1898e-02,  4.4493e-02,  1.3509e-02,  1.1818e-02,\n",
      "                    3.8449e-02, -1.9963e-02, -4.5017e-02, -1.7494e-02,\n",
      "                   -2.3513e-02,  1.5127e-01, -2.3736e-03,  1.5825e-02,\n",
      "                   -1.1494e-02,  2.7316e-02, -3.2026e-02, -4.8816e-02,\n",
      "                   -1.6358e-02,  3.7486e-02, -1.4331e-02, -1.0996e-02,\n",
      "                   -1.3119e-02,  1.1624e-02,  6.1865e-03, -1.1870e-02,\n",
      "                   -1.2480e-01,  2.2036e-02,  5.7722e-02,  5.7893e-03,\n",
      "                   -2.5620e-02, -9.3855e-04,  1.7419e-01, -2.3321e-02,\n",
      "                    6.6001e-03,  4.9130e-03, -1.2926e-02, -2.1571e-02,\n",
      "                   -3.5244e-02,  2.1553e-02, -1.2742e-02, -1.1647e-02,\n",
      "                    4.2977e-02, -3.5081e-03, -1.5782e-03,  3.4751e-03,\n",
      "                    1.0494e-02,  3.3371e-02, -6.2874e-03,  3.7456e-02,\n",
      "                    2.1623e-02, -3.3057e-02,  4.7196e-02, -3.5934e-03,\n",
      "                    6.9505e-02,  7.9341e-03, -4.4347e-02,  5.2879e-03,\n",
      "                    2.7867e-02,  2.8834e-02, -1.6204e-02,  2.0173e-02,\n",
      "                   -1.4923e-02,  5.1003e-02, -2.6871e-02, -5.4602e-03,\n",
      "                   -1.1414e-02, -4.5312e-03,  2.2961e-02,  6.0595e-02,\n",
      "                   -9.6073e-04,  7.1419e-03,  2.5573e-02,  4.4384e-02,\n",
      "                    6.2514e-03,  1.6933e-02,  3.5098e-02, -5.4735e-02,\n",
      "                    3.7360e-02, -1.4087e-02,  4.2034e-02, -5.2099e-04,\n",
      "                   -1.0541e-02, -8.1869e-03, -5.0790e-02,  3.0245e-02,\n",
      "                   -1.4994e-02, -3.1704e-02, -4.5692e-02,  4.1587e-02,\n",
      "                   -9.3186e-03, -2.8074e-02,  9.3807e-03, -1.6009e-02,\n",
      "                    3.4029e-02,  3.5687e-02,  6.4752e-02,  1.3521e-02,\n",
      "                   -3.8268e-02, -7.1120e-03, -3.7033e-02,  1.6828e-02,\n",
      "                   -2.9145e-03,  3.8182e-03, -2.2682e-02, -4.1806e-03,\n",
      "                    8.3804e-03, -1.8996e-02, -8.3426e-03, -1.6710e-02,\n",
      "                   -3.1059e-02, -8.6691e-04, -1.4774e-01,  1.4482e-02,\n",
      "                   -5.8925e-02,  4.3915e-02, -3.1753e-02,  1.7156e-02,\n",
      "                    3.2346e-02, -1.2515e-01,  3.9068e-03, -4.6039e-03,\n",
      "                    3.4248e-03, -8.3096e-02, -1.9655e-02, -2.5084e-02,\n",
      "                   -2.8041e-03, -7.9627e-02, -7.3887e-03, -2.1636e-02,\n",
      "                    8.2516e-03, -4.4092e-02,  1.9706e-02, -3.4590e-02,\n",
      "                   -1.3657e-02,  1.0594e-02, -8.9585e-03,  7.6071e-03,\n",
      "                   -1.8283e-02,  1.6737e-02,  3.2414e-02, -2.0840e-02,\n",
      "                    2.1320e-02, -3.5152e-03, -3.5801e-02,  1.8467e-02,\n",
      "                    3.7819e-04, -9.7997e-02, -5.5929e-02,  1.0648e-02,\n",
      "                   -9.6306e-03, -2.7766e-02, -4.3177e-02,  2.9886e-02,\n",
      "                   -2.6216e-03, -2.6918e-02, -1.9547e-02, -8.1844e-02,\n",
      "                   -9.1091e-03, -3.8188e-02,  4.5440e-02, -9.4416e-03,\n",
      "                    2.9004e-02, -2.3522e-02, -3.5815e-02,  1.5195e-02,\n",
      "                    1.2646e-02, -9.6116e-03,  1.6400e-02, -1.2215e-02,\n",
      "                    1.4649e-02,  6.7685e-03, -4.2185e-02, -1.2607e-02,\n",
      "                    3.6718e-02, -2.0346e-02, -1.5398e-02,  1.5071e-02,\n",
      "                    3.2574e-02, -4.0065e-02,  2.5264e-02,  7.6440e-02,\n",
      "                    2.8158e-02, -3.3557e-02,  2.0107e-02, -6.9724e-03,\n",
      "                    8.8476e-04,  5.4399e-02, -9.1861e-03,  1.7375e-02,\n",
      "                   -6.7635e-03, -3.0841e-02, -2.7913e-03,  1.0289e-02,\n",
      "                   -1.5099e-01,  1.5902e-02,  2.2821e-02,  7.6120e-02,\n",
      "                   -3.1078e-02,  2.4487e-02,  3.5779e-02, -2.7441e-02,\n",
      "                   -4.0818e-02, -1.0127e-02,  1.1503e-02, -8.1984e-03,\n",
      "                   -5.8802e-03, -5.4109e-03,  3.4183e-04,  3.1197e-02,\n",
      "                    2.4835e-02,  2.9627e-03, -3.0087e-02, -2.7090e-03,\n",
      "                    2.2253e-02,  1.7118e-03,  1.8856e-02, -2.1287e-02,\n",
      "                    2.6804e-02, -3.0462e-02,  3.4311e-02,  1.6836e-02,\n",
      "                    1.3528e-03,  7.5709e-03,  3.9446e-02,  1.5182e-02,\n",
      "                    2.2697e-02,  3.6680e-02,  9.1741e-02,  2.0392e-02,\n",
      "                    5.4849e-03,  3.0687e-02,  3.3891e-02,  3.3209e-02,\n",
      "                    1.7168e-03,  2.2763e-02,  2.3515e-02, -1.1664e-02,\n",
      "                   -3.1860e-02,  6.5443e-03,  3.7355e-02, -4.4889e-02,\n",
      "                   -2.8904e-02,  2.5150e-02, -7.1493e-02,  5.6301e-02,\n",
      "                    5.1935e-02,  7.7838e-03,  4.8615e-02, -2.2593e-02,\n",
      "                   -1.0640e-02,  1.4859e-02, -2.1628e-02, -5.8986e-02,\n",
      "                    3.2151e-02, -2.2761e-03, -4.8399e-03, -4.4312e-02,\n",
      "                    4.2685e-02, -2.5218e-02,  2.7039e-02, -1.7210e-02,\n",
      "                   -1.9061e-02, -8.5251e-03, -2.7533e-03,  4.9013e-02,\n",
      "                    3.2615e-02, -2.1343e-02, -1.2150e-03,  2.3160e-02,\n",
      "                   -1.9211e-02,  7.2895e-02,  8.8943e-03, -1.5201e-02,\n",
      "                    4.3792e-03,  3.9362e-02,  1.1740e-02, -1.1065e-02,\n",
      "                    3.1799e-02,  4.1151e-02, -1.9270e-02, -4.2584e-02,\n",
      "                   -6.7262e-02,  5.5480e-02, -9.0042e-03,  2.6952e-02,\n",
      "                    2.2888e-02,  5.3463e-02,  3.3232e-03, -1.6713e-02,\n",
      "                   -3.5584e-03,  1.8641e-02, -8.7500e-04,  2.0507e-02,\n",
      "                    4.9137e-02, -9.4999e-02,  1.6329e-02, -2.1680e-03,\n",
      "                    1.5509e-01,  5.3197e-02,  3.4786e-02,  1.3320e-02,\n",
      "                   -2.2614e-02,  2.9804e-02,  2.4582e-02, -1.3190e-02,\n",
      "                   -5.6100e-03, -6.6100e-03,  5.0961e-02, -1.5332e-02,\n",
      "                   -5.2796e-03, -4.8243e-03, -1.8818e-05,  3.7419e-03,\n",
      "                    1.2652e-03, -4.5202e-02, -3.2769e-02,  5.6363e-02,\n",
      "                   -2.3744e-02, -5.2062e-03,  5.5326e-02, -2.5608e-03,\n",
      "                    5.3802e-02, -3.7082e-02,  2.8441e-02, -1.9977e-02,\n",
      "                   -5.5744e-02, -3.7144e-02, -4.1255e-02, -6.3290e-04,\n",
      "                    1.6464e-02,  1.9056e-02, -5.2318e-02, -2.3812e-02,\n",
      "                    1.5077e-02, -6.2861e-02, -7.0815e-03, -1.0273e-02,\n",
      "                   -3.4999e-03, -7.9623e-02, -1.1400e-03,  2.8303e-02,\n",
      "                    3.2992e-02,  2.9267e-02,  8.0196e-03,  1.5502e-03,\n",
      "                    2.6597e-02,  2.4718e-02, -4.9572e-02, -1.6435e-02,\n",
      "                   -3.2739e-02,  3.1301e-02, -1.1993e-02, -2.5359e-02,\n",
      "                   -4.3060e-02,  3.4789e-03,  3.2668e-02,  4.1823e-03,\n",
      "                   -4.3434e-02, -6.3579e-02, -2.2839e-03, -2.7337e-02,\n",
      "                   -8.4071e-03,  4.6040e-02, -3.4302e-03,  3.0778e-03,\n",
      "                   -2.0945e-02,  1.6116e-03,  3.1760e-03, -2.9638e-02,\n",
      "                   -3.7857e-02,  2.4153e-03, -4.1305e-04,  3.3647e-02,\n",
      "                   -3.8517e-02,  1.7911e-02,  1.2850e-02, -1.2193e-02,\n",
      "                    9.1725e-03,  2.1833e-02,  1.7378e-02, -7.3286e-03,\n",
      "                    7.5089e-02, -9.8420e-03, -3.9134e-03, -3.1963e-02,\n",
      "                   -3.8753e-02, -6.3764e-02, -2.1159e-02,  3.0256e-02,\n",
      "                   -3.3217e-02,  6.3829e-02,  1.1589e-02, -3.6610e-02,\n",
      "                    2.7327e-02, -9.3812e-02, -1.8315e-02,  5.2338e-02,\n",
      "                    4.0284e-02, -2.3497e-03,  3.4612e-02,  6.8455e-03,\n",
      "                   -1.5803e-02, -1.2691e-03, -3.0532e-03, -5.5553e-02,\n",
      "                   -3.3815e-02,  1.1414e-02,  2.0079e-02,  8.5502e-03,\n",
      "                    1.8923e-02,  2.7064e-02, -2.2748e-03,  1.0481e-02,\n",
      "                   -1.0614e-02, -2.8585e-02,  4.4281e-03,  9.7684e-03,\n",
      "                    2.2668e-02,  7.1026e-02,  4.2650e-04, -1.2505e-02,\n",
      "                   -5.3746e-02, -7.2237e-03, -1.7860e-02, -1.9689e-03,\n",
      "                    4.6715e-03,  4.5291e-03, -4.9591e-02,  1.6709e-02,\n",
      "                   -7.2887e-03, -1.9859e-02,  9.7644e-03,  2.8181e-02,\n",
      "                    3.4621e-02, -1.4987e-02, -1.6865e-01, -1.3521e-02,\n",
      "                    1.3765e-02, -1.7963e-02,  3.5461e-02,  3.3229e-03,\n",
      "                   -1.4631e-02, -2.8898e-02, -1.8656e-02,  4.7362e-03,\n",
      "                   -8.6638e-03, -1.7860e-02, -4.5306e-02,  1.6334e-02,\n",
      "                    8.5357e-03,  1.6319e-02, -5.0954e-02, -2.0617e-02,\n",
      "                    5.9073e-02,  5.2947e-02, -8.2047e-03, -2.5344e-02,\n",
      "                    8.4640e-03,  4.5030e-02, -4.3402e-02,  5.3050e-02,\n",
      "                    3.2057e-02, -9.2266e-03,  4.5414e-03,  2.0032e-02,\n",
      "                   -5.8217e-02,  1.3833e-02, -2.7739e-02,  3.3811e-04,\n",
      "                    2.5080e-02,  2.2761e-02, -5.7253e-03,  2.3282e-03,\n",
      "                   -7.2087e-02,  7.3006e-02, -2.8789e-02, -2.4654e-02,\n",
      "                   -2.9375e-02,  1.5910e-02, -4.7229e-02, -3.2530e-03,\n",
      "                   -2.8928e-02,  8.1059e-03, -3.3140e-02,  2.7734e-02,\n",
      "                    1.2344e-02,  1.7171e-02,  1.2156e-02,  2.2192e-02,\n",
      "                    4.7956e-02, -1.8461e-02, -1.1822e-03,  1.5366e-02,\n",
      "                    1.9076e-03,  1.9273e-02, -2.8705e-03, -1.1685e-03,\n",
      "                    2.3136e-02, -2.1577e-02,  4.4120e-02, -2.1899e-02,\n",
      "                    2.6769e-02, -6.7015e-03, -2.3992e-02, -4.6541e-03,\n",
      "                   -6.6981e-03,  3.3484e-02, -2.7285e-02, -1.1567e-02,\n",
      "                   -3.9479e-03, -1.0037e-02, -5.0722e-03, -2.3124e-02,\n",
      "                   -1.1841e-02,  1.6623e-02,  7.6050e-03,  7.7952e-03,\n",
      "                    5.9988e-03,  2.5364e-02,  4.6141e-03, -1.5281e-03,\n",
      "                    1.7921e-02,  1.0010e-01, -1.5910e-02, -1.5157e-02,\n",
      "                   -3.3672e-02, -4.2741e-02, -6.6456e-03, -2.0086e-02,\n",
      "                   -5.1061e-02,  2.7765e-02,  1.3441e-02, -1.7011e-02,\n",
      "                    3.4667e-02,  3.0639e-03,  4.6830e-03,  3.7398e-03,\n",
      "                   -2.4661e-02,  3.8479e-02, -1.4389e-02, -2.7506e-02,\n",
      "                    2.6101e-02, -3.1267e-02, -1.2420e-02,  6.6871e-03,\n",
      "                    3.3803e-02,  5.2643e-02, -1.7860e-02, -1.0210e-02,\n",
      "                    2.5956e-02, -1.6909e-02,  1.0424e-03,  1.3504e-02,\n",
      "                   -6.4431e-02, -2.7029e-02,  2.8753e-03, -3.4582e-03,\n",
      "                   -5.9899e-02, -2.0825e-02,  1.5185e-02, -4.8888e-03,\n",
      "                    1.1521e-02, -2.8974e-02, -4.1488e-02, -8.2376e-03,\n",
      "                    2.7512e-02,  1.9310e-02,  4.7066e-02, -1.0710e-02,\n",
      "                    9.6010e-03,  3.9387e-03,  1.4221e-02,  7.8027e-03,\n",
      "                    2.8600e-02, -2.5638e-02, -1.3555e-02, -2.1367e-02,\n",
      "                   -1.0734e-02, -1.7220e-02,  1.2121e-03, -3.8728e-03,\n",
      "                   -3.5323e-02, -3.3471e-02,  9.2417e-03,  1.6534e-02,\n",
      "                   -3.9159e-03,  1.0193e-02, -3.0783e-02, -3.7213e-02,\n",
      "                   -2.8277e-02, -3.4270e-02, -9.4901e-03, -3.0117e-02,\n",
      "                    4.0414e-02,  2.1289e-02, -7.8037e-04, -2.6492e-02,\n",
      "                    2.2012e-02], dtype=torch.float32, requires_grad=True))\n",
      "Radius: 0.6319634307032679\n",
      "Distances to center: tensor([0.6013, 0.5801, 0.5732,  ..., 0.5911, 0.5958, 0.6067],\n",
      "       dtype=torch.float32, grad_fn=<MulBackward0>)\n",
      "Max distance: 0.7796358466148376\n",
      "Radius: 0.6319634307032679\n",
      "Number of points inside radius: 151562.0/158700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.LorentzHyperbolicSVDD at 0x704a18cdd690>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curvature = 2.3026\n",
    "epochs = 50\n",
    "\n",
    "# fit the SVDD model on the benign points\n",
    "test_svdd_fit(hyper_points=benign_points, curvature=curvature, nu=0.05, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e4868b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of malicious points: torch.Size([158700, 768])\n"
     ]
    }
   ],
   "source": [
    "# get only the points whose class is 'malicious'\n",
    "malicious_points = []\n",
    "for point in hyperbolic_points:\n",
    "    if point[1] == \"malicious\":\n",
    "        malicious_points.append(point[0])\n",
    "\n",
    "\n",
    "malicious_points = torch.stack(malicious_points)\n",
    "print(f\"Number of malicious points: {malicious_points.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6816c553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the time component to the malicious points\n",
    "curvature = 2.3026\n",
    "malicious_points = torch.cat(\n",
    "    [\n",
    "        torch.sqrt(\n",
    "            1 / curvature + torch.sum(malicious_points**2, dim=-1, keepdim=True)\n",
    "        ),\n",
    "        malicious_points,\n",
    "    ],\n",
    "    dim=-1,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3130ba16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Malicious points after adding time component: torch.Size([158700, 769])\n"
     ]
    }
   ],
   "source": [
    "print(\"Malicious points after adding time component:\", malicious_points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a02d0e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before fit:\n",
      "Mean center before adding time component: torch.Size([768])\n",
      "data after adding time component: torch.Size([158700, 769])\n",
      "Epoch [1/50], Avg Loss: 0.4026, Points inside radius (minibatch stats): 147419/158700, center norm: 1.9162, radius: 0.6141\n",
      "Epoch [2/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147293/158700, center norm: 1.9219, radius: 0.6137\n",
      "Epoch [3/50], Avg Loss: 0.4009, Points inside radius (minibatch stats): 147348/158700, center norm: 1.9126, radius: 0.5922\n",
      "Epoch [4/50], Avg Loss: 0.4009, Points inside radius (minibatch stats): 147491/158700, center norm: 1.9065, radius: 0.6022\n",
      "Epoch [5/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147278/158700, center norm: 1.9148, radius: 0.6081\n",
      "Epoch [6/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147279/158700, center norm: 1.9211, radius: 0.6050\n",
      "Epoch [7/50], Avg Loss: 0.4007, Points inside radius (minibatch stats): 147308/158700, center norm: 1.9156, radius: 0.6006\n",
      "Epoch [8/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147280/158700, center norm: 1.9058, radius: 0.6045\n",
      "Epoch [9/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147255/158700, center norm: 1.9027, radius: 0.6021\n",
      "Epoch [10/50], Avg Loss: 0.4009, Points inside radius (minibatch stats): 147469/158700, center norm: 1.9037, radius: 0.6048\n",
      "Epoch [11/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147395/158700, center norm: 1.9120, radius: 0.6149\n",
      "Epoch [12/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147407/158700, center norm: 1.9042, radius: 0.6092\n",
      "Epoch [13/50], Avg Loss: 0.4007, Points inside radius (minibatch stats): 147380/158700, center norm: 1.9021, radius: 0.6093\n",
      "Epoch [14/50], Avg Loss: 0.4007, Points inside radius (minibatch stats): 147467/158700, center norm: 1.9050, radius: 0.6325\n",
      "Epoch [15/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147327/158700, center norm: 1.9003, radius: 0.6058\n",
      "Epoch [16/50], Avg Loss: 0.4009, Points inside radius (minibatch stats): 147422/158700, center norm: 1.9056, radius: 0.6016\n",
      "Epoch [17/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147239/158700, center norm: 1.9237, radius: 0.6178\n",
      "Epoch [18/50], Avg Loss: 0.4007, Points inside radius (minibatch stats): 147288/158700, center norm: 1.9109, radius: 0.5996\n",
      "Epoch [19/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147355/158700, center norm: 1.9167, radius: 0.6112\n",
      "Epoch [20/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147299/158700, center norm: 1.9113, radius: 0.6210\n",
      "Epoch [21/50], Avg Loss: 0.4007, Points inside radius (minibatch stats): 147407/158700, center norm: 1.8985, radius: 0.6093\n",
      "Epoch [22/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147396/158700, center norm: 1.9269, radius: 0.6111\n",
      "Epoch [23/50], Avg Loss: 0.4007, Points inside radius (minibatch stats): 147379/158700, center norm: 1.9173, radius: 0.6108\n",
      "Epoch [24/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147369/158700, center norm: 1.9149, radius: 0.6190\n",
      "Epoch [25/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147397/158700, center norm: 1.9031, radius: 0.5943\n",
      "Epoch [26/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147417/158700, center norm: 1.9076, radius: 0.6061\n",
      "Epoch [27/50], Avg Loss: 0.4007, Points inside radius (minibatch stats): 147221/158700, center norm: 1.8998, radius: 0.5910\n",
      "Epoch [28/50], Avg Loss: 0.4007, Points inside radius (minibatch stats): 147353/158700, center norm: 1.9168, radius: 0.6083\n",
      "Epoch [29/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147198/158700, center norm: 1.9057, radius: 0.6093\n",
      "Epoch [30/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147322/158700, center norm: 1.9046, radius: 0.6204\n",
      "Epoch [31/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147313/158700, center norm: 1.9128, radius: 0.5983\n",
      "Epoch [32/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147196/158700, center norm: 1.9073, radius: 0.6298\n",
      "Epoch [33/50], Avg Loss: 0.4009, Points inside radius (minibatch stats): 147375/158700, center norm: 1.8933, radius: 0.6183\n",
      "Epoch [34/50], Avg Loss: 0.4009, Points inside radius (minibatch stats): 147299/158700, center norm: 1.9029, radius: 0.5956\n",
      "Epoch [35/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147273/158700, center norm: 1.9045, radius: 0.6005\n",
      "Epoch [36/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147464/158700, center norm: 1.9087, radius: 0.5881\n",
      "Epoch [37/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147356/158700, center norm: 1.9081, radius: 0.6079\n",
      "Epoch [38/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147380/158700, center norm: 1.8985, radius: 0.6003\n",
      "Epoch [39/50], Avg Loss: 0.4006, Points inside radius (minibatch stats): 147340/158700, center norm: 1.8959, radius: 0.6133\n",
      "Epoch [40/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147352/158700, center norm: 1.9304, radius: 0.6213\n",
      "Epoch [41/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147207/158700, center norm: 1.8996, radius: 0.6021\n",
      "Epoch [42/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147212/158700, center norm: 1.9082, radius: 0.6094\n",
      "Epoch [43/50], Avg Loss: 0.4009, Points inside radius (minibatch stats): 147272/158700, center norm: 1.9272, radius: 0.6156\n",
      "Epoch [44/50], Avg Loss: 0.4009, Points inside radius (minibatch stats): 147417/158700, center norm: 1.9014, radius: 0.6114\n",
      "Epoch [45/50], Avg Loss: 0.4007, Points inside radius (minibatch stats): 147322/158700, center norm: 1.9070, radius: 0.6084\n",
      "Epoch [46/50], Avg Loss: 0.4007, Points inside radius (minibatch stats): 147476/158700, center norm: 1.9257, radius: 0.6028\n",
      "Epoch [47/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147313/158700, center norm: 1.9049, radius: 0.6067\n",
      "Epoch [48/50], Avg Loss: 0.4007, Points inside radius (minibatch stats): 147286/158700, center norm: 1.9025, radius: 0.6031\n",
      "Epoch [49/50], Avg Loss: 0.4009, Points inside radius (minibatch stats): 147460/158700, center norm: 1.9028, radius: 0.6286\n",
      "Epoch [50/50], Avg Loss: 0.4008, Points inside radius (minibatch stats): 147252/158700, center norm: 1.9118, radius: 0.5968\n",
      "After fit:\n",
      "Center: Parameter on Lorentz manifold containing:\n",
      "Parameter(ManifoldParameter([ 1.7259e+00, -2.0450e-02, -5.0087e-03,  2.0237e-02,\n",
      "                    4.0738e-02,  1.7320e-02,  1.1197e-02,  1.6884e-02,\n",
      "                   -2.0696e-02, -1.2699e-02, -5.0315e-03,  2.5864e-02,\n",
      "                   -1.9230e-02, -9.4171e-03, -8.5203e-03,  1.7131e-02,\n",
      "                   -3.4648e-02,  3.3713e-02, -5.5020e-04,  6.2373e-03,\n",
      "                   -2.6888e-02,  4.5658e-04, -1.2327e-02, -2.8499e-03,\n",
      "                    4.4902e-02, -4.6462e-02,  1.1228e-02,  1.0497e-03,\n",
      "                    1.6576e-02,  4.0986e-02,  3.3232e-03, -1.0858e-03,\n",
      "                    1.1838e-02,  1.3616e-03, -1.0296e-02,  7.3530e-03,\n",
      "                   -5.3199e-03, -1.3288e-02, -2.7214e-03,  6.9314e-03,\n",
      "                   -9.4277e-03,  4.1497e-02, -3.1022e-02,  1.7038e-02,\n",
      "                   -5.4394e-02,  2.5385e-02, -2.2028e-02,  3.5390e-02,\n",
      "                    2.4064e-03, -1.2653e-02, -8.0574e-02, -1.4864e-02,\n",
      "                    2.1500e-03,  1.0348e-02,  1.7436e-02,  2.0763e-02,\n",
      "                    9.8384e-03,  5.9249e-03,  1.6836e-02,  4.2434e-03,\n",
      "                   -9.1076e-03,  1.6594e-02,  3.4516e-02, -1.1533e-02,\n",
      "                    2.0777e-02,  6.3726e-03,  1.7705e-02,  4.8233e-03,\n",
      "                    5.2004e-03, -5.7622e-07, -2.3481e-02,  1.0751e-02,\n",
      "                    3.5110e-02,  5.3066e-03, -1.0930e-02,  8.0443e-03,\n",
      "                   -1.2074e-03,  1.1159e-02,  3.1162e-03,  8.3796e-03,\n",
      "                   -1.3120e-02,  4.0864e-02, -1.0242e-02,  2.0106e-02,\n",
      "                   -1.1553e-02,  2.0934e-02,  5.4943e-03, -3.5122e-02,\n",
      "                    1.7122e-03,  2.3600e-02,  3.6697e-02, -1.1549e-03,\n",
      "                   -7.8060e-03,  1.1399e-03, -3.9205e-02,  3.3857e-03,\n",
      "                   -2.2467e-03,  2.9812e-02, -3.8531e-02,  2.9434e-02,\n",
      "                   -2.0592e-02, -3.8255e-02,  2.3016e-02, -1.6204e-02,\n",
      "                    4.4672e-03, -3.5493e-02,  9.0596e-03, -1.8443e-02,\n",
      "                   -2.6627e-02, -2.2360e-03, -2.2682e-02,  3.8565e-02,\n",
      "                    1.0667e-03, -9.5485e-03, -4.0195e-03, -1.6265e-02,\n",
      "                   -2.2485e-02, -1.0809e-02, -7.3684e-03, -1.4989e-02,\n",
      "                   -6.0681e-03,  3.6766e-02, -7.0147e-03,  2.0326e-03,\n",
      "                    1.1663e-02, -1.6374e-03, -2.7511e-02,  1.6241e-02,\n",
      "                    1.6500e-02,  1.8502e-02, -1.2565e-02,  1.6452e-03,\n",
      "                    2.9432e-02,  8.3325e-04, -1.1562e-02,  1.9273e-02,\n",
      "                   -1.7140e-02, -2.4858e-02,  4.1516e-03,  7.5599e-03,\n",
      "                    2.0592e-03,  1.6312e-02, -2.7871e-02,  2.3251e-02,\n",
      "                   -3.8091e-02, -1.3665e-02,  1.6243e-02,  5.0799e-02,\n",
      "                   -4.4465e-02, -3.3713e-02,  2.5811e-02,  8.4317e-03,\n",
      "                    2.5937e-02,  4.3156e-02,  2.2931e-02,  3.2792e-02,\n",
      "                    3.4121e-02,  1.8617e-02, -3.6327e-04,  1.8086e-02,\n",
      "                   -1.8472e-02,  1.7573e-02,  5.7167e-04,  1.7831e-02,\n",
      "                    4.8670e-02,  3.3409e-02,  5.2138e-03, -3.6864e-02,\n",
      "                   -4.2707e-02, -5.3833e-03, -1.2508e-03,  4.4089e-03,\n",
      "                   -2.9113e-02,  3.9013e-02,  1.3289e-02,  1.4122e-02,\n",
      "                    3.1500e-02, -1.8884e-02, -4.2531e-02, -1.6696e-02,\n",
      "                   -2.1884e-02,  1.3661e-01, -2.3154e-03,  2.0902e-02,\n",
      "                   -1.2279e-02,  2.7356e-02, -3.1716e-02, -4.4551e-02,\n",
      "                   -1.5250e-02,  3.8524e-02, -1.2265e-02, -1.1326e-02,\n",
      "                   -1.4983e-02,  3.9805e-03,  2.3864e-03, -1.0252e-02,\n",
      "                   -1.1163e-01,  2.1561e-02,  5.4333e-02,  9.5736e-03,\n",
      "                   -2.2131e-02,  2.3188e-03,  1.5678e-01, -2.4844e-02,\n",
      "                    9.3458e-03,  7.5627e-03, -1.0665e-02, -1.9458e-02,\n",
      "                   -2.8602e-02,  2.7562e-02, -1.7727e-02, -8.7179e-03,\n",
      "                    4.0902e-02, -3.5501e-03, -9.5194e-04,  1.9647e-03,\n",
      "                    1.3660e-02,  3.2173e-02, -3.1756e-03,  3.0406e-02,\n",
      "                    1.7761e-02, -3.0529e-02,  4.4893e-02, -7.3094e-03,\n",
      "                    6.5312e-02,  3.4211e-03, -3.9284e-02,  3.3806e-03,\n",
      "                    2.5098e-02,  2.7339e-02, -1.7115e-02,  1.4636e-02,\n",
      "                   -1.3306e-02,  4.5069e-02, -2.6101e-02, -3.1745e-03,\n",
      "                   -1.0115e-02, -4.2670e-03,  2.0220e-02,  5.6312e-02,\n",
      "                    2.6247e-03,  1.2186e-02,  1.6749e-02,  4.3360e-02,\n",
      "                    4.3826e-03,  1.3234e-02,  3.5194e-02, -5.4405e-02,\n",
      "                    3.4442e-02, -1.3047e-02,  3.6253e-02, -3.8765e-03,\n",
      "                   -1.0646e-02, -6.8580e-03, -4.4673e-02,  3.2429e-02,\n",
      "                   -1.5223e-02, -3.1436e-02, -4.4751e-02,  4.0325e-02,\n",
      "                   -5.8481e-03, -2.0482e-02,  7.8158e-03, -1.7669e-02,\n",
      "                    2.7126e-02,  3.0230e-02,  5.6010e-02,  1.5759e-02,\n",
      "                   -3.5586e-02, -5.9988e-03, -2.8569e-02,  1.5860e-02,\n",
      "                   -3.9199e-03,  4.2145e-03, -2.4265e-02, -2.4774e-03,\n",
      "                    5.1234e-03, -1.7234e-02, -1.0837e-02, -1.1356e-02,\n",
      "                   -2.6068e-02, -1.7359e-03, -1.3774e-01,  1.1306e-02,\n",
      "                   -5.0572e-02,  4.0380e-02, -3.3251e-02,  1.6936e-02,\n",
      "                    2.7517e-02, -1.1038e-01, -3.6656e-03, -4.2565e-03,\n",
      "                    6.7536e-03, -7.6228e-02, -2.1268e-02, -2.0032e-02,\n",
      "                   -5.7621e-03, -7.2900e-02, -7.9278e-03, -1.9143e-02,\n",
      "                    9.1134e-03, -3.5394e-02,  1.9479e-02, -3.5410e-02,\n",
      "                   -1.2137e-02,  8.7002e-03, -8.4486e-03,  6.4084e-03,\n",
      "                   -1.5023e-02,  2.0320e-02,  3.3371e-02, -2.3906e-02,\n",
      "                    2.2208e-02, -6.0480e-03, -2.8165e-02,  1.7175e-02,\n",
      "                    3.7981e-03, -8.2713e-02, -5.3046e-02,  8.8467e-03,\n",
      "                   -7.6137e-03, -2.1105e-02, -3.9010e-02,  2.9335e-02,\n",
      "                    1.1849e-04, -2.1872e-02, -1.9292e-02, -6.9868e-02,\n",
      "                   -4.6639e-03, -3.4912e-02,  3.7163e-02, -6.7180e-03,\n",
      "                    2.2731e-02, -2.5173e-02, -3.5599e-02,  1.3510e-02,\n",
      "                    1.0688e-02, -7.1719e-03,  1.3702e-02, -1.2807e-02,\n",
      "                    7.3792e-03,  3.3495e-03, -3.4511e-02, -1.2146e-02,\n",
      "                    3.0576e-02, -2.2843e-02, -1.4375e-02,  1.2773e-02,\n",
      "                    3.1847e-02, -3.8777e-02,  2.5184e-02,  7.3640e-02,\n",
      "                    2.8448e-02, -3.1366e-02,  2.1513e-02, -6.5411e-03,\n",
      "                   -5.3340e-04,  4.7062e-02, -7.1741e-03,  1.3284e-02,\n",
      "                   -3.3850e-03, -3.1938e-02, -2.2745e-03,  7.3249e-03,\n",
      "                   -1.3412e-01,  9.6866e-03,  2.0925e-02,  7.5581e-02,\n",
      "                   -3.1529e-02,  2.0052e-02,  2.9178e-02, -2.2381e-02,\n",
      "                   -3.4929e-02, -1.0887e-02,  1.6585e-02, -7.3180e-03,\n",
      "                   -8.6336e-03, -4.5748e-03,  5.2588e-03,  2.7236e-02,\n",
      "                    2.2317e-02,  8.0179e-03, -2.9047e-02, -4.1633e-03,\n",
      "                    2.0208e-02,  2.9620e-03,  2.0247e-02, -1.7135e-02,\n",
      "                    2.4438e-02, -3.0036e-02,  2.9511e-02,  1.8324e-02,\n",
      "                    3.8055e-03,  5.7770e-03,  4.2117e-02,  1.0386e-02,\n",
      "                    2.3935e-02,  3.5067e-02,  8.0777e-02,  1.5316e-02,\n",
      "                    6.2037e-03,  2.8030e-02,  2.7532e-02,  2.9428e-02,\n",
      "                    2.5814e-04,  1.5998e-02,  2.0184e-02, -7.9806e-03,\n",
      "                   -3.2538e-02,  3.3983e-03,  3.2169e-02, -4.3643e-02,\n",
      "                   -2.6526e-02,  2.4232e-02, -6.5162e-02,  5.1027e-02,\n",
      "                    5.2460e-02,  5.5586e-03,  4.0611e-02, -2.5123e-02,\n",
      "                   -1.3581e-02,  1.1207e-02, -1.5778e-02, -5.3014e-02,\n",
      "                    2.8334e-02, -5.7117e-03, -3.9780e-03, -4.0685e-02,\n",
      "                    3.6978e-02, -2.3431e-02,  2.5711e-02, -1.4990e-02,\n",
      "                   -1.5241e-02, -1.1898e-02, -1.9541e-03,  4.5878e-02,\n",
      "                    3.0121e-02, -2.3327e-02, -2.8999e-04,  2.6645e-02,\n",
      "                   -2.1747e-02,  6.3495e-02,  4.8516e-03, -1.4173e-02,\n",
      "                    6.8360e-03,  3.4157e-02,  1.1937e-02, -1.0688e-02,\n",
      "                    2.8699e-02,  3.3849e-02, -1.6653e-02, -3.6883e-02,\n",
      "                   -6.1523e-02,  4.3103e-02, -1.2157e-02,  2.3779e-02,\n",
      "                    1.8306e-02,  4.8915e-02,  1.4142e-03, -1.4773e-02,\n",
      "                   -3.3225e-03,  1.4828e-02, -4.1775e-03,  1.9961e-02,\n",
      "                    4.8648e-02, -9.0801e-02,  1.2607e-02, -3.9984e-03,\n",
      "                    1.3686e-01,  4.7664e-02,  3.4321e-02,  9.7398e-03,\n",
      "                   -2.0710e-02,  2.5640e-02,  2.9391e-02, -1.6140e-02,\n",
      "                   -6.0792e-03, -5.9704e-03,  4.0696e-02, -1.0429e-02,\n",
      "                   -6.1529e-03, -6.0813e-03, -1.5498e-03,  6.3537e-03,\n",
      "                    3.8743e-03, -4.0121e-02, -3.1284e-02,  5.2177e-02,\n",
      "                   -1.9351e-02, -6.5089e-03,  4.9277e-02, -4.5008e-03,\n",
      "                    5.0074e-02, -3.4755e-02,  3.0583e-02, -1.5833e-02,\n",
      "                   -4.8909e-02, -3.5167e-02, -3.7313e-02,  1.4143e-03,\n",
      "                    1.0120e-02,  1.8746e-02, -4.6278e-02, -1.8939e-02,\n",
      "                    1.2820e-02, -5.9983e-02, -1.0363e-02, -1.0837e-03,\n",
      "                    2.4531e-03, -7.7307e-02,  1.3734e-03,  2.0387e-02,\n",
      "                    3.2986e-02,  2.6670e-02,  7.4562e-03,  4.8483e-03,\n",
      "                    2.3242e-02,  2.1495e-02, -4.5460e-02, -1.6577e-02,\n",
      "                   -3.2979e-02,  3.0313e-02, -9.6527e-03, -2.1898e-02,\n",
      "                   -3.7889e-02,  3.5844e-03,  3.0127e-02, -1.1744e-03,\n",
      "                   -3.8164e-02, -6.2045e-02, -3.6544e-03, -2.5888e-02,\n",
      "                   -1.0706e-02,  3.6092e-02, -3.8003e-03, -2.3495e-03,\n",
      "                   -2.0726e-02, -7.7140e-04,  9.0491e-04, -3.0254e-02,\n",
      "                   -3.9135e-02, -7.3419e-04,  6.3528e-03,  2.6404e-02,\n",
      "                   -2.9659e-02,  1.6190e-02,  8.7801e-03, -8.9273e-03,\n",
      "                    5.7235e-03,  2.0857e-02,  1.2929e-02, -6.8089e-03,\n",
      "                    6.2509e-02, -7.7283e-03, -3.7203e-03, -2.5758e-02,\n",
      "                   -3.4975e-02, -5.9146e-02, -2.0763e-02,  2.5594e-02,\n",
      "                   -2.8667e-02,  6.1665e-02,  1.1681e-02, -2.9953e-02,\n",
      "                    2.3407e-02, -9.0110e-02, -2.0459e-02,  4.7320e-02,\n",
      "                    3.5969e-02, -4.7365e-04,  3.2729e-02,  1.0045e-02,\n",
      "                   -1.2645e-02, -2.3407e-03, -3.6529e-03, -5.4393e-02,\n",
      "                   -3.2441e-02,  1.0426e-02,  1.4347e-02,  1.0150e-02,\n",
      "                    1.9685e-02,  2.2375e-02, -2.4430e-03,  8.6934e-03,\n",
      "                   -1.0687e-02, -2.5705e-02,  7.3434e-03,  1.0774e-02,\n",
      "                    2.0784e-02,  6.5242e-02,  5.0475e-03, -1.2584e-02,\n",
      "                   -4.9202e-02, -5.2223e-03, -7.3174e-03, -5.1399e-04,\n",
      "                    5.5569e-03,  2.0866e-03, -4.2443e-02,  1.5633e-02,\n",
      "                   -9.6283e-03, -1.8630e-02,  7.1038e-03,  2.5777e-02,\n",
      "                    2.9887e-02, -1.3600e-02, -1.5375e-01, -9.6125e-03,\n",
      "                    1.1027e-02, -1.4959e-02,  3.4063e-02,  2.1673e-03,\n",
      "                   -1.4194e-02, -2.3482e-02, -1.7115e-02,  5.1483e-03,\n",
      "                   -9.3857e-03, -1.6049e-02, -4.3965e-02,  1.5424e-02,\n",
      "                    5.9790e-03,  1.3677e-02, -4.5158e-02, -1.8850e-02,\n",
      "                    5.2546e-02,  5.1080e-02, -9.7992e-03, -3.2020e-02,\n",
      "                    9.1179e-03,  3.9440e-02, -4.0405e-02,  4.5830e-02,\n",
      "                    3.3498e-02, -8.2178e-03,  5.7966e-03,  1.0002e-02,\n",
      "                   -6.1763e-02,  9.1654e-03, -2.2518e-02, -2.2385e-03,\n",
      "                    2.3702e-02,  2.2915e-02, -6.9873e-03,  3.5292e-03,\n",
      "                   -6.3716e-02,  6.6630e-02, -2.5224e-02, -2.1126e-02,\n",
      "                   -2.2342e-02,  1.3715e-02, -4.3805e-02, -1.6140e-03,\n",
      "                   -2.6493e-02,  6.9982e-03, -2.9596e-02,  2.7009e-02,\n",
      "                    4.6417e-03,  1.6171e-02,  1.0557e-02,  2.0663e-02,\n",
      "                    5.0731e-02, -1.7834e-02, -8.5990e-04,  1.3562e-02,\n",
      "                   -3.4016e-03,  1.7877e-02,  2.0417e-03,  1.2930e-03,\n",
      "                    1.9708e-02, -2.2827e-02,  4.1517e-02, -1.4913e-02,\n",
      "                    2.2731e-02, -1.1071e-02, -2.2499e-02, -3.4688e-03,\n",
      "                   -1.4519e-03,  2.9541e-02, -2.9519e-02, -1.4940e-02,\n",
      "                   -5.3635e-03, -9.7021e-03, -2.8232e-03, -2.1720e-02,\n",
      "                   -1.2618e-02,  2.0804e-02,  9.2284e-03,  8.7488e-03,\n",
      "                    2.8172e-03,  2.6923e-02,  1.4726e-03, -3.7558e-03,\n",
      "                    1.7042e-02,  9.1724e-02, -1.4036e-02, -1.3427e-02,\n",
      "                   -3.0644e-02, -4.2611e-02, -5.0456e-03, -1.6450e-02,\n",
      "                   -4.7443e-02,  2.5799e-02,  1.7411e-02, -1.9286e-02,\n",
      "                    3.2135e-02,  4.1203e-03,  4.1476e-03,  6.0606e-03,\n",
      "                   -2.5875e-02,  3.5557e-02, -1.5002e-02, -2.5156e-02,\n",
      "                    1.8124e-02, -2.6521e-02, -7.3654e-03,  8.5990e-03,\n",
      "                    2.9754e-02,  4.4915e-02, -1.3374e-02, -5.6478e-03,\n",
      "                    2.2357e-02, -1.4075e-02,  2.2266e-03,  1.4408e-02,\n",
      "                   -5.3861e-02, -2.4314e-02,  9.3917e-04, -2.0770e-03,\n",
      "                   -5.6408e-02, -1.7855e-02,  1.0608e-02, -9.3420e-03,\n",
      "                    6.4438e-03, -2.4161e-02, -3.4499e-02, -9.1669e-03,\n",
      "                    2.9676e-02,  1.8347e-02,  4.1257e-02, -1.1680e-02,\n",
      "                    8.7073e-03,  1.5058e-03,  1.6752e-02,  7.6700e-03,\n",
      "                    2.7055e-02, -2.7007e-02, -1.1939e-02, -1.8776e-02,\n",
      "                   -1.1825e-02, -1.9120e-02,  1.4314e-03, -2.2654e-03,\n",
      "                   -3.6684e-02, -3.4204e-02,  1.0058e-02,  1.4297e-02,\n",
      "                   -9.9003e-03,  1.1985e-02, -3.2666e-02, -3.8746e-02,\n",
      "                   -2.6908e-02, -3.1016e-02, -6.8052e-03, -2.7221e-02,\n",
      "                    3.2751e-02,  1.9478e-02, -9.7240e-04, -2.5544e-02,\n",
      "                    2.1407e-02], dtype=torch.float32, requires_grad=True))\n",
      "Radius: 0.5967698999988477\n",
      "Distances to center: tensor([0.6016, 0.5670, 0.5660,  ..., 0.5804, 0.5821, 0.6049],\n",
      "       dtype=torch.float32, grad_fn=<MulBackward0>)\n",
      "Max distance: 0.8085998296737671\n",
      "Radius: 0.5967698999988477\n",
      "Number of points inside radius: 132261.0/158700\n",
      "Malicious predictions: tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# get the model trained on benign points and predict on malicious points\n",
    "benign_model = test_svdd_fit(\n",
    "    hyper_points=benign_points, curvature=curvature, nu=0.1, epochs=epochs\n",
    ")\n",
    "# predict on malicious points\n",
    "malicious_predictions = benign_model.predict(malicious_points)\n",
    "print(f\"Malicious predictions: {malicious_predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1febd24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of malicious points classified as benign: 2522\n",
      "Accuracy on malicious points: 0.9841\n",
      "Benign predictions: tensor([0, 1, 1,  ..., 1, 1, 0], dtype=torch.int32)\n",
      "Number of benign points classified as benign: 132261\n",
      "Accuracy on benign points: 0.8334\n"
     ]
    }
   ],
   "source": [
    "# print the number of malicious points classified as benign\n",
    "num_malicious_benign = (malicious_predictions == 1).sum().item()\n",
    "print(f\"Number of malicious points classified as benign: {num_malicious_benign}\")\n",
    "# print the accuracy of the model on malicious points\n",
    "accuracy_malicious = (malicious_predictions == 0).sum().item() / malicious_predictions.shape[0]\n",
    "print(f\"Accuracy on malicious points: {accuracy_malicious:.4f}\")\n",
    "# get the model trained on benign points and predict on beign points\n",
    "benign_points_with_time = torch.cat(\n",
    "    [\n",
    "        torch.sqrt(\n",
    "            1 / curvature + torch.sum(benign_points**2, dim=-1, keepdim=True)\n",
    "        ),\n",
    "        benign_points,\n",
    "    ],\n",
    "    dim=-1,\n",
    ")\n",
    "\n",
    "benign_predictions = benign_model.predict(benign_points_with_time)\n",
    "print(f\"Benign predictions: {benign_predictions}\")\n",
    "# print the number of benign points classified as benign\n",
    "num_benign_benign = (benign_predictions == 1).sum().item()\n",
    "print(f\"Number of benign points classified as benign: {num_benign_benign}\")\n",
    "# print the accuracy of the model on benign points\n",
    "accuracy_benign = (benign_predictions == 1).sum().item() / benign_predictions.shape[0]\n",
    "print(f\"Accuracy on benign points: {accuracy_benign:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf7f7cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before fit:\n",
      "Mean center before adding time component: torch.Size([768])\n",
      "data after adding time component: torch.Size([158700, 769])\n",
      "Epoch [1/50], Avg Loss: 1.0000, center: 1.6047, radius: 1.0000, inside: 158698/158700, center_grad_norm: 0.0000, radius_grad_norm: 309.9609\n",
      "Epoch [2/50], Avg Loss: 1.0000, center: 1.6050, radius: 1.0000, inside: 158699/158700, center_grad_norm: 0.0000, radius_grad_norm: 619.9219\n",
      "Epoch [3/50], Avg Loss: 1.0000, center: 1.6051, radius: 1.0000, inside: 158700/158700, center_grad_norm: 0.0000, radius_grad_norm: 929.9023\n",
      "Epoch [4/50], Avg Loss: 1.0000, center: 1.6051, radius: 1.0000, inside: 158700/158700, center_grad_norm: 0.0000, radius_grad_norm: 1239.9023\n",
      "Epoch [5/50], Avg Loss: 1.0000, center: 1.6051, radius: 1.0000, inside: 158700/158700, center_grad_norm: 0.0000, radius_grad_norm: 1549.9023\n",
      "Epoch [6/50], Avg Loss: 1.0000, center: 1.6051, radius: 1.0000, inside: 158700/158700, center_grad_norm: 0.0000, radius_grad_norm: 1859.9023\n",
      "Epoch [7/50], Avg Loss: 1.0000, center: 1.6051, radius: 1.0000, inside: 158700/158700, center_grad_norm: 0.0000, radius_grad_norm: 2169.9023\n",
      "Epoch [8/50], Avg Loss: 1.0000, center: 1.6051, radius: 1.0000, inside: 158700/158700, center_grad_norm: 0.0000, radius_grad_norm: 2479.9023\n",
      "Epoch [9/50], Avg Loss: 1.0000, center: 1.6051, radius: 1.0000, inside: 158700/158700, center_grad_norm: 0.0000, radius_grad_norm: 2789.9023\n",
      "Epoch [10/50], Avg Loss: 1.0000, center: 1.6051, radius: 1.0000, inside: 158700/158700, center_grad_norm: 0.0000, radius_grad_norm: 3099.9023\n",
      "Epoch [11/50], Avg Loss: 0.6086, center: 1.6051, radius: 0.7211, inside: 144485/158700, center_grad_norm: 556.3762, radius_grad_norm: 0.0373\n",
      "Epoch [12/50], Avg Loss: 0.5879, center: 1.6051, radius: 0.7175, inside: 142892/158700, center_grad_norm: 1175.8115, radius_grad_norm: 0.2783\n",
      "Epoch [13/50], Avg Loss: 0.5878, center: 1.6051, radius: 0.7223, inside: 142827/158700, center_grad_norm: 1797.5078, radius_grad_norm: 0.1665\n",
      "Epoch [14/50], Avg Loss: 0.5879, center: 1.6051, radius: 0.7203, inside: 142960/158700, center_grad_norm: 2417.3633, radius_grad_norm: 0.2361\n",
      "Epoch [15/50], Avg Loss: 0.5879, center: 1.6051, radius: 0.7198, inside: 142824/158700, center_grad_norm: 3037.5952, radius_grad_norm: 0.0631\n",
      "Epoch [16/50], Avg Loss: 0.5330, center: 1.8621, radius: 0.7198, inside: 156616/158700, center_grad_norm: 0.2767, radius_grad_norm: 192.6067\n",
      "Epoch [17/50], Avg Loss: 0.5192, center: 1.9202, radius: 0.7198, inside: 158103/158700, center_grad_norm: 0.1389, radius_grad_norm: 407.2386\n",
      "Epoch [18/50], Avg Loss: 0.5186, center: 1.9462, radius: 0.7198, inside: 158380/158700, center_grad_norm: 0.1850, radius_grad_norm: 625.8069\n",
      "Epoch [19/50], Avg Loss: 0.5184, center: 1.9623, radius: 0.7198, inside: 158477/158700, center_grad_norm: 0.0483, radius_grad_norm: 845.7820\n",
      "Epoch [20/50], Avg Loss: 0.5184, center: 1.9739, radius: 0.7198, inside: 158524/158700, center_grad_norm: 0.0933, radius_grad_norm: 1066.4035\n",
      "Epoch [21/50], Avg Loss: 0.5183, center: 1.9829, radius: 0.7198, inside: 158551/158700, center_grad_norm: 0.0475, radius_grad_norm: 1287.4050\n",
      "Epoch [22/50], Avg Loss: 0.5183, center: 1.9898, radius: 0.7198, inside: 158574/158700, center_grad_norm: 0.0481, radius_grad_norm: 1508.7579\n",
      "Epoch [23/50], Avg Loss: 0.5183, center: 1.9957, radius: 0.7198, inside: 158584/158700, center_grad_norm: 0.0467, radius_grad_norm: 1730.2514\n",
      "Epoch [24/50], Avg Loss: 0.5183, center: 2.0008, radius: 0.7198, inside: 158595/158700, center_grad_norm: 0.0477, radius_grad_norm: 1951.8855\n",
      "Epoch [25/50], Avg Loss: 0.5183, center: 2.0054, radius: 0.7198, inside: 158601/158700, center_grad_norm: 0.0922, radius_grad_norm: 2173.6318\n",
      "Epoch [26/50], Avg Loss: 0.4290, center: 2.0054, radius: 0.6375, inside: 143366/158700, center_grad_norm: 595.5486, radius_grad_norm: 0.0686\n",
      "Epoch [27/50], Avg Loss: 0.4273, center: 2.0054, radius: 0.6378, inside: 142891/158700, center_grad_norm: 1215.1503, radius_grad_norm: 0.1069\n",
      "Epoch [28/50], Avg Loss: 0.4273, center: 2.0054, radius: 0.6377, inside: 142839/158700, center_grad_norm: 1834.6577, radius_grad_norm: 0.1707\n",
      "Epoch [29/50], Avg Loss: 0.4273, center: 2.0054, radius: 0.6377, inside: 142888/158700, center_grad_norm: 2454.1978, radius_grad_norm: 0.0559\n",
      "Epoch [30/50], Avg Loss: 0.4273, center: 2.0054, radius: 0.6383, inside: 142945/158700, center_grad_norm: 3073.9136, radius_grad_norm: 0.1598\n",
      "Epoch [31/50], Avg Loss: 0.4181, center: 1.9820, radius: 0.6383, inside: 153737/158700, center_grad_norm: 1.6012, radius_grad_norm: 134.1376\n",
      "Epoch [32/50], Avg Loss: 0.4176, center: 1.9832, radius: 0.6383, inside: 154222/158700, center_grad_norm: 1.1737, radius_grad_norm: 275.4439\n",
      "Epoch [33/50], Avg Loss: 0.4176, center: 1.9838, radius: 0.6383, inside: 154228/158700, center_grad_norm: 1.0232, radius_grad_norm: 417.1252\n",
      "Epoch [34/50], Avg Loss: 0.4176, center: 1.9832, radius: 0.6383, inside: 154223/158700, center_grad_norm: 1.0618, radius_grad_norm: 558.6068\n",
      "Epoch [35/50], Avg Loss: 0.4176, center: 1.9829, radius: 0.6383, inside: 154230/158700, center_grad_norm: 1.1265, radius_grad_norm: 700.1502\n",
      "Epoch [36/50], Avg Loss: 0.4176, center: 1.9827, radius: 0.6383, inside: 154221/158700, center_grad_norm: 1.6272, radius_grad_norm: 841.7031\n",
      "Epoch [37/50], Avg Loss: 0.4176, center: 1.9843, radius: 0.6383, inside: 154242/158700, center_grad_norm: 1.3215, radius_grad_norm: 983.5946\n",
      "Epoch [38/50], Avg Loss: 0.4176, center: 1.9823, radius: 0.6383, inside: 154206/158700, center_grad_norm: 1.2231, radius_grad_norm: 1124.8882\n",
      "Epoch [39/50], Avg Loss: 0.4176, center: 1.9830, radius: 0.6383, inside: 154230/158700, center_grad_norm: 1.2692, radius_grad_norm: 1266.6428\n",
      "Epoch [40/50], Avg Loss: 0.4176, center: 1.9813, radius: 0.6383, inside: 154193/158700, center_grad_norm: 1.0936, radius_grad_norm: 1407.9621\n",
      "Epoch [41/50], Avg Loss: 0.4069, center: 1.9813, radius: 0.6199, inside: 142941/158700, center_grad_norm: 616.0657, radius_grad_norm: 0.2533\n",
      "Epoch [42/50], Avg Loss: 0.4069, center: 1.9813, radius: 0.6179, inside: 142925/158700, center_grad_norm: 1237.8951, radius_grad_norm: 0.1159\n",
      "Epoch [43/50], Avg Loss: 0.4069, center: 1.9813, radius: 0.6208, inside: 142913/158700, center_grad_norm: 1861.3203, radius_grad_norm: 0.2046\n",
      "Epoch [44/50], Avg Loss: 0.4069, center: 1.9813, radius: 0.6175, inside: 142903/158700, center_grad_norm: 2482.7827, radius_grad_norm: 0.2892\n",
      "Epoch [45/50], Avg Loss: 0.4069, center: 1.9813, radius: 0.6195, inside: 142906/158700, center_grad_norm: 3105.8892, radius_grad_norm: 0.1286\n",
      "Epoch [46/50], Avg Loss: 0.4030, center: 1.9427, radius: 0.6195, inside: 150538/158700, center_grad_norm: 2.2208, radius_grad_norm: 91.4567\n",
      "Epoch [47/50], Avg Loss: 0.4028, center: 1.9410, radius: 0.6195, inside: 151018/158700, center_grad_norm: 1.5986, radius_grad_norm: 189.3224\n",
      "Epoch [48/50], Avg Loss: 0.4028, center: 1.9414, radius: 0.6195, inside: 151026/158700, center_grad_norm: 2.1166, radius_grad_norm: 287.5844\n",
      "Epoch [49/50], Avg Loss: 0.4028, center: 1.9408, radius: 0.6195, inside: 151013/158700, center_grad_norm: 2.0415, radius_grad_norm: 385.7619\n",
      "Epoch [50/50], Avg Loss: 0.4028, center: 1.9427, radius: 0.6195, inside: 151046/158700, center_grad_norm: 2.1786, radius_grad_norm: 484.2773\n",
      "After fit:\n",
      "Center: Parameter on Lorentz manifold containing:\n",
      "Parameter(ManifoldParameter([ 1.7431e+00, -1.8798e-02, -6.9592e-03,  1.8674e-02,\n",
      "                    4.3673e-02,  1.6191e-02,  1.0751e-02,  1.6554e-02,\n",
      "                   -1.9335e-02, -1.3455e-02, -4.1550e-03,  2.4706e-02,\n",
      "                   -2.2780e-02, -1.1224e-02, -9.3176e-03,  1.4763e-02,\n",
      "                   -3.5025e-02,  3.1017e-02, -1.0899e-03,  8.2645e-03,\n",
      "                   -3.2645e-02,  3.4112e-04, -1.3412e-02, -1.1626e-03,\n",
      "                    4.2810e-02, -5.2427e-02,  1.0967e-02,  3.3034e-03,\n",
      "                    1.6915e-02,  4.1547e-02,  2.5666e-03, -4.3993e-03,\n",
      "                    1.2319e-02, -8.0291e-04, -9.2817e-03,  4.0308e-03,\n",
      "                   -7.3271e-03, -1.5938e-02, -1.2758e-03,  5.1720e-03,\n",
      "                   -9.6185e-03,  4.0943e-02, -3.2178e-02,  2.1997e-02,\n",
      "                   -5.6311e-02,  2.6322e-02, -2.3280e-02,  3.9092e-02,\n",
      "                    1.4764e-03, -1.4949e-02, -8.5937e-02, -1.5685e-02,\n",
      "                    2.3525e-03,  1.1920e-02,  1.5902e-02,  2.1901e-02,\n",
      "                    9.0113e-03,  3.4160e-03,  1.7658e-02,  4.7273e-03,\n",
      "                   -6.2986e-03,  1.8928e-02,  3.3607e-02, -1.0089e-02,\n",
      "                    1.9895e-02,  6.2335e-03,  1.7898e-02,  7.3304e-03,\n",
      "                    7.8349e-03, -7.5647e-04, -2.4424e-02,  9.7492e-03,\n",
      "                    3.7056e-02,  4.9889e-03, -1.0154e-02,  9.3535e-03,\n",
      "                   -2.1616e-03,  8.9350e-03, -3.4015e-04,  1.0263e-02,\n",
      "                   -1.6449e-02,  4.6721e-02, -1.3139e-02,  2.0397e-02,\n",
      "                   -8.9349e-03,  2.0051e-02,  6.4477e-03, -3.7532e-02,\n",
      "                    2.2429e-04,  2.3108e-02,  3.5268e-02, -5.1827e-04,\n",
      "                   -7.6388e-03,  1.2006e-04, -3.9788e-02,  3.6807e-03,\n",
      "                   -4.5989e-03,  3.4817e-02, -3.8926e-02,  2.8972e-02,\n",
      "                   -2.1167e-02, -4.1534e-02,  2.3378e-02, -1.4135e-02,\n",
      "                    2.1160e-03, -3.5835e-02,  9.0206e-03, -2.1081e-02,\n",
      "                   -2.5908e-02, -1.8775e-03, -2.5029e-02,  3.9354e-02,\n",
      "                    3.7687e-03, -9.1167e-03, -2.8525e-03, -1.6470e-02,\n",
      "                   -2.1621e-02, -1.1743e-02, -5.5958e-03, -1.3119e-02,\n",
      "                   -4.2601e-03,  4.1047e-02, -7.0194e-03,  1.0770e-03,\n",
      "                    1.2820e-02, -1.3431e-03, -3.1231e-02,  1.9154e-02,\n",
      "                    1.5142e-02,  2.0236e-02, -1.2581e-02,  1.7776e-03,\n",
      "                    2.9953e-02, -1.5816e-03, -1.3687e-02,  2.0401e-02,\n",
      "                   -1.6757e-02, -2.3008e-02,  5.6373e-03,  7.8060e-03,\n",
      "                    1.3363e-03,  2.0140e-02, -2.5187e-02,  2.5201e-02,\n",
      "                   -3.8037e-02, -1.2684e-02,  2.0586e-02,  5.1416e-02,\n",
      "                   -4.4644e-02, -3.3852e-02,  2.8022e-02,  9.2294e-03,\n",
      "                    2.5426e-02,  4.5366e-02,  2.5451e-02,  3.2901e-02,\n",
      "                    3.6624e-02,  1.9022e-02,  4.2178e-03,  1.8038e-02,\n",
      "                   -1.7290e-02,  1.6885e-02,  2.5085e-04,  1.9934e-02,\n",
      "                    5.1071e-02,  3.3956e-02,  8.3513e-03, -4.0318e-02,\n",
      "                   -4.4353e-02, -3.6007e-03,  4.7654e-04,  5.0599e-03,\n",
      "                   -2.9324e-02,  4.2207e-02,  1.1836e-02,  1.3291e-02,\n",
      "                    3.3362e-02, -1.7690e-02, -4.2938e-02, -2.0810e-02,\n",
      "                   -2.1299e-02,  1.4337e-01, -2.6768e-03,  1.6835e-02,\n",
      "                   -1.3135e-02,  2.5364e-02, -3.5671e-02, -4.7249e-02,\n",
      "                   -1.4142e-02,  3.8600e-02, -1.3505e-02, -1.3344e-02,\n",
      "                   -1.4273e-02,  7.5787e-03,  3.6089e-03, -1.2077e-02,\n",
      "                   -1.1712e-01,  2.1427e-02,  5.5405e-02,  9.3043e-03,\n",
      "                   -2.2778e-02,  9.7130e-04,  1.6538e-01, -2.5129e-02,\n",
      "                    7.9376e-03,  6.8200e-03, -1.1996e-02, -2.0863e-02,\n",
      "                   -3.0742e-02,  2.3532e-02, -1.5108e-02, -1.1316e-02,\n",
      "                    4.2561e-02, -6.1007e-03,  4.0467e-04,  2.5306e-03,\n",
      "                    1.2226e-02,  3.4460e-02, -5.5368e-03,  3.4270e-02,\n",
      "                    1.7654e-02, -3.2080e-02,  4.4642e-02, -6.5699e-03,\n",
      "                    6.7476e-02,  5.9622e-03, -4.2092e-02,  3.0625e-03,\n",
      "                    2.9333e-02,  2.7555e-02, -1.6875e-02,  1.8534e-02,\n",
      "                   -1.6321e-02,  4.5127e-02, -2.5053e-02, -3.0744e-03,\n",
      "                   -1.0634e-02, -4.1136e-03,  2.2406e-02,  6.1337e-02,\n",
      "                   -2.0499e-04,  9.8716e-03,  1.9376e-02,  4.3529e-02,\n",
      "                    6.2094e-03,  1.5494e-02,  3.4770e-02, -5.5943e-02,\n",
      "                    3.6903e-02, -1.3051e-02,  3.7534e-02, -3.4022e-03,\n",
      "                   -1.0878e-02, -7.7954e-03, -4.5820e-02,  3.0644e-02,\n",
      "                   -1.5767e-02, -3.1245e-02, -4.7304e-02,  4.2131e-02,\n",
      "                   -7.3080e-03, -2.4986e-02,  8.3011e-03, -1.7756e-02,\n",
      "                    2.9417e-02,  3.3850e-02,  5.8073e-02,  1.2416e-02,\n",
      "                   -3.7529e-02, -4.6044e-03, -3.3780e-02,  1.5954e-02,\n",
      "                   -3.7669e-03,  3.5230e-03, -2.4885e-02, -1.0789e-03,\n",
      "                    7.2602e-03, -1.9835e-02, -1.1125e-02, -1.2681e-02,\n",
      "                   -2.7218e-02, -3.2810e-04, -1.4379e-01,  1.3347e-02,\n",
      "                   -5.2508e-02,  4.4593e-02, -3.2575e-02,  1.9500e-02,\n",
      "                    2.8674e-02, -1.1715e-01,  2.9619e-03, -3.3887e-03,\n",
      "                    5.6398e-03, -7.8285e-02, -2.0909e-02, -2.2258e-02,\n",
      "                   -2.7474e-03, -7.6165e-02, -9.0931e-03, -1.9440e-02,\n",
      "                    8.7884e-03, -4.0051e-02,  2.0533e-02, -3.7055e-02,\n",
      "                   -1.0954e-02,  1.0763e-02, -9.0476e-03,  7.9438e-03,\n",
      "                   -1.5945e-02,  1.7231e-02,  3.1377e-02, -1.9691e-02,\n",
      "                    2.2959e-02, -4.2785e-03, -3.3869e-02,  1.5833e-02,\n",
      "                    4.9104e-04, -8.6873e-02, -5.7728e-02,  1.1974e-02,\n",
      "                   -7.6426e-03, -2.2573e-02, -4.1700e-02,  3.0687e-02,\n",
      "                    2.6455e-04, -2.3302e-02, -2.0121e-02, -7.3295e-02,\n",
      "                   -6.3452e-03, -3.5855e-02,  3.7787e-02, -7.5872e-03,\n",
      "                    2.5781e-02, -2.5746e-02, -3.5733e-02,  1.3165e-02,\n",
      "                    1.3530e-02, -8.1732e-03,  1.4696e-02, -1.1642e-02,\n",
      "                    9.0213e-03,  5.7003e-03, -3.8533e-02, -1.2721e-02,\n",
      "                    3.1517e-02, -2.1269e-02, -1.5673e-02,  1.3738e-02,\n",
      "                    3.3752e-02, -3.8964e-02,  2.3250e-02,  7.7203e-02,\n",
      "                    2.9186e-02, -3.3211e-02,  1.9547e-02, -7.0095e-03,\n",
      "                    7.3525e-04,  5.1131e-02, -8.4386e-03,  1.5660e-02,\n",
      "                   -6.3644e-03, -3.0580e-02, -2.0486e-03,  8.5306e-03,\n",
      "                   -1.4303e-01,  9.3903e-03,  2.4593e-02,  7.8626e-02,\n",
      "                   -2.9875e-02,  2.2475e-02,  3.0747e-02, -2.3820e-02,\n",
      "                   -3.7518e-02, -1.0441e-02,  1.1801e-02, -8.6604e-03,\n",
      "                   -5.4664e-03, -5.8081e-03,  2.2949e-03,  2.9838e-02,\n",
      "                    2.2658e-02,  8.9948e-03, -2.9897e-02, -3.5752e-03,\n",
      "                    2.1968e-02,  4.0402e-03,  1.9340e-02, -2.1194e-02,\n",
      "                    2.6221e-02, -3.1707e-02,  3.1210e-02,  1.5888e-02,\n",
      "                    4.7092e-03,  8.4564e-03,  4.0454e-02,  1.2407e-02,\n",
      "                    2.0485e-02,  3.1498e-02,  8.7862e-02,  1.7724e-02,\n",
      "                    6.4479e-03,  2.9615e-02,  2.9381e-02,  3.2142e-02,\n",
      "                   -5.5464e-04,  1.5568e-02,  1.9953e-02, -8.6746e-03,\n",
      "                   -3.1813e-02,  7.2668e-03,  3.5666e-02, -4.5960e-02,\n",
      "                   -2.9376e-02,  2.4367e-02, -6.8095e-02,  5.3433e-02,\n",
      "                    5.4208e-02,  6.6446e-03,  4.3619e-02, -2.5012e-02,\n",
      "                   -1.0770e-02,  1.1565e-02, -1.8917e-02, -5.4402e-02,\n",
      "                    2.9878e-02, -2.5780e-03, -3.6777e-03, -4.3099e-02,\n",
      "                    4.0174e-02, -2.3534e-02,  2.7235e-02, -1.4103e-02,\n",
      "                   -1.6133e-02, -1.0515e-02, -3.1427e-03,  4.6388e-02,\n",
      "                    3.1596e-02, -2.2586e-02, -3.0645e-04,  2.5081e-02,\n",
      "                   -2.1733e-02,  6.5425e-02,  8.0168e-03, -1.3468e-02,\n",
      "                    5.8235e-03,  3.9113e-02,  1.0908e-02, -1.0327e-02,\n",
      "                    3.1094e-02,  3.7289e-02, -1.6949e-02, -4.0941e-02,\n",
      "                   -6.4405e-02,  4.5772e-02, -9.0899e-03,  2.3257e-02,\n",
      "                    2.2540e-02,  5.2202e-02,  9.6773e-04, -1.5745e-02,\n",
      "                   -2.6660e-03,  1.7443e-02, -4.1025e-03,  1.9678e-02,\n",
      "                    4.9484e-02, -8.8727e-02,  1.6478e-02, -2.5460e-03,\n",
      "                    1.4583e-01,  5.2625e-02,  3.2385e-02,  1.1588e-02,\n",
      "                   -2.0137e-02,  2.7539e-02,  2.5764e-02, -1.5568e-02,\n",
      "                   -3.7987e-03, -7.1662e-03,  4.2764e-02, -1.0030e-02,\n",
      "                   -7.3141e-03, -4.2566e-03, -1.3381e-03,  7.7937e-03,\n",
      "                    3.5577e-03, -4.3967e-02, -3.2074e-02,  5.2763e-02,\n",
      "                   -1.8793e-02, -6.8713e-03,  5.1893e-02, -2.4754e-03,\n",
      "                    5.0260e-02, -3.6813e-02,  2.9913e-02, -1.7358e-02,\n",
      "                   -5.1922e-02, -3.4330e-02, -3.8512e-02,  1.4359e-03,\n",
      "                    1.0685e-02,  1.7880e-02, -4.9522e-02, -2.3549e-02,\n",
      "                    1.4094e-02, -5.9969e-02, -7.2780e-03, -7.2875e-03,\n",
      "                   -2.0187e-03, -7.7807e-02,  6.7099e-04,  2.6609e-02,\n",
      "                    3.4022e-02,  2.7334e-02,  7.7107e-03,  3.0770e-03,\n",
      "                    2.5227e-02,  2.2316e-02, -4.6088e-02, -1.7589e-02,\n",
      "                   -3.4688e-02,  2.9749e-02, -1.0588e-02, -2.4509e-02,\n",
      "                   -4.1014e-02,  4.0631e-03,  3.2512e-02, -8.3724e-04,\n",
      "                   -4.3014e-02, -6.0983e-02, -5.5434e-04, -2.9788e-02,\n",
      "                   -1.0819e-02,  3.8164e-02, -1.3123e-03, -7.5661e-04,\n",
      "                   -1.9672e-02, -4.7713e-04, -7.6009e-05, -3.2688e-02,\n",
      "                   -3.7495e-02,  1.1561e-03,  1.0938e-03,  3.1529e-02,\n",
      "                   -3.2075e-02,  1.6310e-02,  1.0042e-02, -1.0576e-02,\n",
      "                    8.8966e-03,  2.2900e-02,  1.7323e-02, -6.5366e-03,\n",
      "                    6.8400e-02, -8.4010e-03, -3.7327e-03, -2.9599e-02,\n",
      "                   -3.3973e-02, -6.1674e-02, -2.2137e-02,  2.9049e-02,\n",
      "                   -2.9773e-02,  6.4024e-02,  1.1538e-02, -3.0570e-02,\n",
      "                    2.4572e-02, -9.1858e-02, -1.9250e-02,  5.0597e-02,\n",
      "                    3.8119e-02, -7.7185e-04,  3.1073e-02,  7.5406e-03,\n",
      "                   -1.5544e-02, -1.9692e-03, -3.2534e-03, -5.4761e-02,\n",
      "                   -3.3527e-02,  9.6123e-03,  1.6810e-02,  9.5901e-03,\n",
      "                    1.9040e-02,  2.6536e-02, -2.7976e-03,  8.8108e-03,\n",
      "                   -1.0469e-02, -2.8187e-02,  4.9684e-03,  9.8437e-03,\n",
      "                    2.0983e-02,  6.8442e-02,  4.5912e-03, -1.0601e-02,\n",
      "                   -4.9468e-02, -5.0718e-03, -1.0911e-02, -1.9068e-03,\n",
      "                    5.6644e-03,  3.2148e-03, -4.2490e-02,  1.5585e-02,\n",
      "                   -8.4623e-03, -1.7937e-02,  9.6011e-03,  2.6570e-02,\n",
      "                    3.2014e-02, -1.4079e-02, -1.6195e-01, -1.3313e-02,\n",
      "                    1.1768e-02, -1.5305e-02,  3.4344e-02,  4.1044e-03,\n",
      "                   -1.4190e-02, -2.7667e-02, -1.8975e-02,  4.4612e-03,\n",
      "                   -8.0116e-03, -1.4351e-02, -4.6094e-02,  1.3955e-02,\n",
      "                    8.9113e-03,  1.2858e-02, -4.8521e-02, -1.7999e-02,\n",
      "                    5.6847e-02,  5.2305e-02, -1.0235e-02, -2.8790e-02,\n",
      "                    7.2566e-03,  4.4355e-02, -4.1759e-02,  5.0203e-02,\n",
      "                    3.1779e-02, -9.0892e-03,  3.9782e-03,  1.4072e-02,\n",
      "                   -6.2209e-02,  1.0711e-02, -2.4993e-02, -1.3292e-03,\n",
      "                    2.4603e-02,  2.2675e-02, -8.0353e-03,  2.3737e-03,\n",
      "                   -6.8814e-02,  6.9589e-02, -2.8880e-02, -2.2431e-02,\n",
      "                   -2.5057e-02,  1.3518e-02, -4.5902e-02, -4.9952e-03,\n",
      "                   -2.7516e-02,  4.9137e-03, -3.0496e-02,  2.6328e-02,\n",
      "                    9.2454e-03,  1.7736e-02,  1.1295e-02,  2.2589e-02,\n",
      "                    4.9506e-02, -1.8799e-02, -3.3449e-03,  1.2460e-02,\n",
      "                   -1.9694e-03,  1.9037e-02, -9.7564e-04,  1.5246e-04,\n",
      "                    2.0342e-02, -2.1066e-02,  4.3597e-02, -1.9970e-02,\n",
      "                    2.4439e-02, -8.2626e-03, -2.3663e-02, -1.3386e-03,\n",
      "                   -3.4929e-03,  3.0181e-02, -2.8400e-02, -1.1828e-02,\n",
      "                   -7.0811e-03, -1.0987e-02, -3.6847e-03, -2.2864e-02,\n",
      "                   -1.1614e-02,  1.8785e-02,  6.9904e-03,  8.2795e-03,\n",
      "                    2.5887e-03,  2.4232e-02,  2.3175e-03, -1.7266e-03,\n",
      "                    1.7397e-02,  9.6075e-02, -1.4003e-02, -1.3328e-02,\n",
      "                   -3.2391e-02, -4.4800e-02, -5.7396e-03, -2.0979e-02,\n",
      "                   -5.1105e-02,  2.5711e-02,  1.5012e-02, -1.8326e-02,\n",
      "                    3.2382e-02,  4.1863e-03,  3.6739e-03,  5.0178e-03,\n",
      "                   -2.3728e-02,  3.5368e-02, -1.5124e-02, -2.6234e-02,\n",
      "                    2.2327e-02, -2.9763e-02, -1.0156e-02,  6.2882e-03,\n",
      "                    3.2846e-02,  4.7718e-02, -1.5270e-02, -8.9592e-03,\n",
      "                    2.2742e-02, -1.3715e-02,  1.8845e-03,  1.2680e-02,\n",
      "                   -6.0464e-02, -2.5380e-02, -6.5871e-04, -2.5095e-03,\n",
      "                   -5.4990e-02, -2.0339e-02,  1.2643e-02, -7.1222e-03,\n",
      "                    8.1393e-03, -2.3481e-02, -3.8482e-02, -9.4361e-03,\n",
      "                    3.0246e-02,  1.6321e-02,  4.4300e-02, -1.3403e-02,\n",
      "                    7.2048e-03,  2.1773e-03,  1.5134e-02,  6.5135e-03,\n",
      "                    2.9396e-02, -2.6699e-02, -1.2851e-02, -1.8720e-02,\n",
      "                   -1.1134e-02, -1.7415e-02,  1.9948e-03, -2.8424e-03,\n",
      "                   -3.6140e-02, -3.5240e-02,  9.5402e-03,  1.4842e-02,\n",
      "                   -8.0501e-03,  1.0142e-02, -3.4396e-02, -3.8182e-02,\n",
      "                   -2.6725e-02, -3.4040e-02, -8.2571e-03, -2.7827e-02,\n",
      "                    3.8395e-02,  2.0238e-02, -2.4210e-03, -2.5949e-02,\n",
      "                    2.1117e-02], dtype=torch.float32, requires_grad=True))\n",
      "Radius: 0.6195080931619992\n",
      "Distances to center: tensor([0.5962, 0.5660, 0.5698,  ..., 0.5799, 0.5862, 0.6040],\n",
      "       dtype=torch.float32, grad_fn=<MulBackward0>)\n",
      "Max distance: 0.7919546961784363\n",
      "Radius: 0.6195080931619992\n",
      "Number of points inside radius: 150963.0/158700\n"
     ]
    }
   ],
   "source": [
    "curvature = 2.3026\n",
    "epochs = 50\n",
    "\n",
    "# fit the SVDD model on the benign points\n",
    "benign_alt_model = test_svdd_fit_alternatively(hyper_points=benign_points, curvature=curvature, nu=0.1, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b8d7f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Malicious predictions: tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "malicious_predictions = benign_alt_model.predict(malicious_points)\n",
    "print(f\"Malicious predictions: {malicious_predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc48289f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of malicious points classified as benign: 5845\n",
      "Accuracy on malicious points: 0.9632\n",
      "Benign predictions: tensor([1, 1, 1,  ..., 1, 1, 1], dtype=torch.int32)\n",
      "Number of benign points classified as benign: 150963\n",
      "Accuracy on benign points: 0.9512\n"
     ]
    }
   ],
   "source": [
    "# print the number of malicious points classified as benign\n",
    "num_malicious_benign = (malicious_predictions == 1).sum().item()\n",
    "print(f\"Number of malicious points classified as benign: {num_malicious_benign}\")\n",
    "# print the accuracy of the model on malicious points\n",
    "accuracy_malicious = (malicious_predictions == 0).sum().item() / malicious_predictions.shape[0]\n",
    "print(f\"Accuracy on malicious points: {accuracy_malicious:.4f}\")\n",
    "# get the model trained on benign points and predict on beign points\n",
    "benign_points_with_time = torch.cat(\n",
    "    [\n",
    "        torch.sqrt(\n",
    "            1 / curvature + torch.sum(benign_points**2, dim=-1, keepdim=True)\n",
    "        ),\n",
    "        benign_points,\n",
    "    ],\n",
    "    dim=-1,\n",
    ")\n",
    "\n",
    "benign_predictions = benign_alt_model.predict(benign_points_with_time)\n",
    "print(f\"Benign predictions: {benign_predictions}\")\n",
    "# print the number of benign points classified as benign\n",
    "num_benign_benign = (benign_predictions == 1).sum().item()\n",
    "print(f\"Number of benign points classified as benign: {num_benign_benign}\")\n",
    "# print the accuracy of the model on benign points\n",
    "accuracy_benign = (benign_predictions == 1).sum().item() / benign_predictions.shape[0]\n",
    "print(f\"Accuracy on benign points: {accuracy_benign:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8864b19c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
